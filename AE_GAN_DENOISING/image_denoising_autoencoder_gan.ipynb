{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45370dc3",
   "metadata": {},
   "source": [
    "# üßº Image Demoising b·∫±ng Autoencoder (PyTorch)\n",
    "\n",
    "\n",
    "\n",
    "## Quy tr√¨nh\n",
    "- ƒê·ªçc d·ªØ li·ªáu ·∫£nh theo chu·∫©n `ImageFolder`\n",
    "- Th√™m nhi·ªÖu: **Gaussian (normal)** / **Bernoulli (mask/dropout)** / **Poisson**\n",
    "- Hu·∫•n luy·ªán Autoencoder ƒë·ªÉ **kh·ª≠ nhi·ªÖu**\n",
    "- ƒê√°nh gi√° b·∫±ng **PSNR** v√† **MSE**\n",
    "- L∆∞u ·∫£nh minh ho·∫°: `*_noisy.png`, `*_denoised.png`, `*_clean.png`\n",
    "- L∆∞u checkpoint t·ªët nh·∫•t theo PSNR: `best_autoencoder.pt`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e1d334",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Torch version:\", torch.__version__)\n",
    "print(\"‚úÖ CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faadec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect Kaggle environment\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "print(f\"üîç Kaggle environment detected: {IN_KAGGLE}\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"‚úÖ Running on Kaggle Notebook\")\n",
    "else:\n",
    "    print(\"üíª Running on local machine\")\n",
    "\n",
    "# ==================== KI·ªÇM TRA GPU ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üñ•Ô∏è KI·ªÇM TRA GPU\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} - {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"cudnn Version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"cudnn Enabled: {torch.backends.cudnn.enabled}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU NOT AVAILABLE - Will use CPU (slower!)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================== C·∫§U H√åNH CHO KAGGLE + GPU ====================\n",
    "\n",
    "# 1) ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh cho Kaggle\n",
    "if IN_KAGGLE:\n",
    "    # Tr√™n Kaggle: upload ·∫£nh v√†o Kaggle dataset ho·∫∑c s·ª≠ d·ª•ng working directory\n",
    "    # Option 1: N·∫øu upload dataset v√†o Kaggle, d√πng path n√†y:\n",
    "    # thu_muc_du_lieu = \"/kaggle/input/your-dataset-name/thumbnails\"\n",
    "    \n",
    "    # Option 2: T·∫°o d·ªØ li·ªáu t·ª´ working directory (recommended cho testing)\n",
    "    thu_muc_du_lieu = \"/kaggle/input/thumbnails\"\n",
    "    thu_muc_ket_qua = \"/kaggle/working/outputs_denoise\"\n",
    "    \n",
    "    # ƒê·ªÉ l∆∞u v√† load checkpoint\n",
    "    duong_dan_checkpoint_ae = \"/kaggle/working/best_ae_model.pth\"\n",
    "    duong_dan_checkpoint_gan_g = \"/kaggle/working/best_gan_generator.pth\"\n",
    "    duong_dan_checkpoint_gan_d = \"/kaggle/working/best_gan_discriminator.pth\"\n",
    "    \n",
    "else:\n",
    "    # Tr√™n m√°y local\n",
    "    thu_muc_du_lieu = \"./thumbnails\"\n",
    "    thu_muc_ket_qua = \"./outputs_denoise\"\n",
    "    duong_dan_checkpoint_ae = \"./best_ae_model.pth\"\n",
    "    duong_dan_checkpoint_gan_g = \"./best_gan_generator.pth\"\n",
    "    duong_dan_checkpoint_gan_d = \"./best_gan_discriminator.pth\"\n",
    "\n",
    "# 2) T·ªêI ∆ØU H√ìA GPU - MULTI-GPU SUPPORT\n",
    "use_cuda = torch.cuda.is_available()\n",
    "n_gpus = torch.cuda.device_count() if use_cuda else 0\n",
    "\n",
    "# Batch size t·ªëi ∆∞u cho GPU - TƒÇNG G·∫§P ƒê√îI N·∫æU C√ì 2 GPU\n",
    "if use_cuda:\n",
    "    # GPU c√≥ ƒë·ªß VRAM, d√πng batch size l·ªõn h∆°n\n",
    "    base_batch_size = 32 if \"P100\" in torch.cuda.get_device_name(0) else 16\n",
    "    batch_size = base_batch_size * n_gpus  # Nh√¢n v·ªõi s·ªë GPU\n",
    "    so_worker = 4 * n_gpus  # TƒÉng workers theo s·ªë GPU\n",
    "    pin_memory = True  # Transfer data nhanh h∆°n\n",
    "    \n",
    "    # T·ªëi ∆∞u cuDNN\n",
    "    torch.backends.cudnn.benchmark = True  # T√¨m fastest implementation\n",
    "    torch.backends.cudnn.enabled = True\n",
    "else:\n",
    "    batch_size = 8  # CPU: batch size nh·ªè\n",
    "    so_worker = 0\n",
    "    pin_memory = False\n",
    "\n",
    "# 3) Tham s·ªë hu·∫•n luy·ªán\n",
    "so_epoch = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 4) K√≠ch th∆∞·ªõc ·∫£nh v√† ƒë·ªô r·ªông m·∫°ng\n",
    "kich_thuoc_anh = 128\n",
    "so_kenh_co_so = 32\n",
    "\n",
    "# 5) Nhi·ªÖu\n",
    "loai_nhieu = \"normal\"     # \"normal\" | \"bernoulli\" | \"poisson\"\n",
    "do_manh_nhieu = 0.05      # normal: sigma; bernoulli: t·ªâ l·ªá mask; poisson: m·ª©c noise\n",
    "\n",
    "# 6) Ki·ªÉu m·ª•c ti√™u\n",
    "kieu_muc_tieu = \"clean\"   # \"clean\" = kh·ª≠ nhi·ªÖu | \"noisy\" = identity-noise\n",
    "\n",
    "# 7) Loss\n",
    "ten_loss = \"mse\"          # \"mse\" | \"l1\"\n",
    "\n",
    "# 8) Thi·∫øt b·ªã\n",
    "bat_buoc_cpu = False\n",
    "thiet_bi = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# 9) MULTI-GPU CONFIG\n",
    "use_multi_gpu = n_gpus > 1 and use_cuda\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è C·∫§U H√åNH TRAINING T·ªêI ∆ØU\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Data dir: {thu_muc_du_lieu}\")\n",
    "print(f\"üìÅ Output dir: {thu_muc_ket_qua}\")\n",
    "print(f\"üñ•Ô∏è Device: {thiet_bi}\")\n",
    "print(f\"üéÆ Number of GPUs: {n_gpus}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"üöÄ MULTI-GPU ENABLED: S·ª≠ d·ª•ng {n_gpus} GPUs ƒë·ªìng th·ªùi!\")\n",
    "    print(f\"   GPUs: {[torch.cuda.get_device_name(i) for i in range(n_gpus)]}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Single GPU mode (ch·ªâ d√πng 1 GPU)\")\n",
    "print(f\"üìä Batch size: {batch_size} ({base_batch_size if use_cuda else batch_size} per GPU)\")\n",
    "print(f\"üîÑ Num workers: {so_worker}\")\n",
    "print(f\"üìå Pin memory: {pin_memory}\")\n",
    "print(f\"üöÄ cuDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "print(f\"‚è±Ô∏è Epochs: {so_epoch}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== THI·∫æT L·∫¨P TH∆Ø M·ª§C + THI·∫æT B·ªä ==================\n",
    "thu_muc_ket_qua = Path(thu_muc_ket_qua)\n",
    "thu_muc_ket_qua.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Thi·∫øt b·ªã ƒë√£ ƒë∆∞·ª£c set ·ªü cell tr∆∞·ªõc\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üñ•Ô∏è Thi·∫øt b·ªã: {thiet_bi} {'‚úÖ GPU' if use_cuda else '‚ùå CPU'}\")\n",
    "print(f\"üìÇ D·ªØ li·ªáu: {Path(thu_muc_du_lieu).resolve()}\")\n",
    "print(f\"üìÅ K·∫øt qu·∫£: {thu_muc_ket_qua.resolve()}\")\n",
    "print(f\"üñºÔ∏è K√≠ch th∆∞·ªõc: {kich_thuoc_anh}x{kich_thuoc_anh}\")\n",
    "print(f\"üéØ Nhi·ªÖu: {loai_nhieu} (œÉ={do_manh_nhieu})\")\n",
    "print(f\"üìâ Loss: {ten_loss.upper()}\")\n",
    "print(f\"‚è±Ô∏è Epochs: {so_epoch} | Batch: {batch_size} | Workers: {so_worker}\")\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "duong_dan_du_lieu = Path(thu_muc_du_lieu)\n",
    "if not duong_dan_du_lieu.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è C·∫¢NH B√ÅO: {duong_dan_du_lieu} kh√¥ng t·ªìn t·∫°i!\")\n",
    "    print(f\"üìå B·∫°n c·∫ßn upload dataset ho·∫∑c t·∫°o th∆∞ m·ª•c d·ªØ li·ªáu tr∆∞·ªõc khi ch·∫°y\")\n",
    "    if IN_KAGGLE:\n",
    "        print(\"üí° Tr√™n Kaggle: H√£y upload dataset ho·∫∑c s·ª≠ d·ª•ng 'Add data' t√≠nh nƒÉng\")\n",
    "\n",
    "# B·∫≠t Mixed Precision Training n·∫øu s·ª≠ d·ª•ng GPU (ti·∫øt ki·ªám RAM, nhanh h∆°n)\n",
    "if use_cuda and torch.cuda.get_device_capability(0)[0] >= 7:  # Compute Capability >= 7 (V100, A100, etc)\n",
    "    print(\"\\n‚úÖ Mixed Precision Training ENABLED (amp)\")\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    use_amp = True\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    print(\"\\n‚ùå Mixed Precision Training DISABLED\")\n",
    "    use_amp = False\n",
    "    scaler = None\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4c473",
   "metadata": {},
   "source": [
    "## 2) H√†m ƒë√°nh gi√° (PSNR, MSE) + h√†m th√™m nhi·ªÖu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== H√ÄM ƒê√ÅNH GI√Å & TH√äM NHI·ªÑU ====================\n",
    "\n",
    "def psnr_tensor(x: torch.Tensor, y: torch.Tensor, max_pixel: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"T√≠nh PSNR gi·ªØa 2 tensor (gi·∫£ s·ª≠ gi√° tr·ªã trong [0,1]).\"\"\"\n",
    "    mse = torch.mean((x - y) ** 2)\n",
    "    if mse == 0:\n",
    "        return torch.tensor(float('inf'), device=x.device)\n",
    "    psnr = 20 * torch.log10(torch.tensor(max_pixel, device=x.device) / torch.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def mse_tensor(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"T√≠nh MSE gi·ªØa 2 tensor.\"\"\"\n",
    "    return torch.mean((x - y) ** 2)\n",
    "\n",
    "def them_nhieu(anh: torch.Tensor, loai: str = \"normal\", do_manh: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Th√™m nhi·ªÖu v√†o ·∫£nh - T·ªêI ∆ØU GPU.\"\"\"\n",
    "    if loai == \"normal\":\n",
    "        nhieu = torch.randn_like(anh) * do_manh\n",
    "    elif loai == \"bernoulli\":\n",
    "        nhieu = torch.bernoulli(torch.full_like(anh, do_manh)) - 0.5\n",
    "    elif loai == \"poisson\":\n",
    "        nhieu = (torch.poisson(anh * 255.0 * do_manh) - anh * 255.0 * do_manh) / (255.0 * do_manh)\n",
    "    else:\n",
    "        nhieu = torch.zeros_like(anh)\n",
    "    return torch.clamp(anh + nhieu, 0, 1)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m: psnr_tensor(), mse_tensor(), them_nhieu()\")\n",
    "print(\"‚úÖ T·∫•t c·∫£ h√†m ƒë√£ t·ªëi ∆∞u ƒë·ªÉ ch·∫°y tr·ª±c ti·∫øp tr√™n GPU (tensor operations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d844f98",
   "metadata": {},
   "source": [
    "## 3) Ki·∫øn tr√∫c Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888610f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== KI·∫æN TR√öC AUTOENCODER ====================\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder ƒë·ªÉ kh·ª≠ nhi·ªÖu ·∫£nh.\"\"\"\n",
    "    def __init__(self, so_kenh_vao: int = 3, kenh_co_so: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: Downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(so_kenh_vao, kenh_co_so, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(kenh_co_so, kenh_co_so*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(kenh_co_so*2, kenh_co_so*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(kenh_co_so*4, kenh_co_so*8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so*8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Decoder: Upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(kenh_co_so*8, kenh_co_so*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(kenh_co_so*4, kenh_co_so*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(kenh_co_so*2, kenh_co_so, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(kenh_co_so),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(kenh_co_so, so_kenh_vao, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Output: [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: Encode -> Decode.\"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh Autoencoder\n",
    "mo_hinh = Autoencoder(so_kenh_vao=3, kenh_co_so=so_kenh_co_so).to(thiet_bi)\n",
    "\n",
    "# ============ WRAP V·ªöI DataParallel N·∫æU C√ì NHI·ªÄU GPU ============\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\nüöÄ Wrapping Autoencoder v·ªõi DataParallel ({n_gpus} GPUs)\")\n",
    "    mo_hinh = nn.DataParallel(mo_hinh)\n",
    "    print(f\"‚úÖ Model s·∫Ω ch·∫°y tr√™n: {mo_hinh.device_ids}\")\n",
    "\n",
    "print(\"‚úÖ Autoencoder Architecture:\")\n",
    "print(mo_hinh)\n",
    "print(f\"\\n‚úÖ Model moved to {thiet_bi}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"üéÆ Multi-GPU Training ENABLED: {n_gpus} GPUs ƒë·ªìng th·ªùi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== CHU·∫®N B·ªä D·ªÆ LI·ªÜU ==================\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((kich_thuoc_anh, kich_thuoc_anh)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load d·ªØ li·ªáu t·ª´ ImageFolder\n",
    "try:\n",
    "    dataset_full = ImageFolder(root=thu_muc_du_lieu, transform=transform)\n",
    "    print(f\"‚úÖ ƒê√£ load {len(dataset_full)} ·∫£nh t·ª´ {thu_muc_du_lieu}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi load d·ªØ li·ªáu: {e}\")\n",
    "    print(f\"üìå ƒê·∫£m b·∫£o c·∫•u tr√∫c th∆∞ m·ª•c: {thu_muc_du_lieu}/classA/ v√† classB/\")\n",
    "    dataset_full = None\n",
    "\n",
    "if dataset_full is not None:\n",
    "    # Split train/val (80/20)\n",
    "    n = len(dataset_full)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = n - n_train\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    indices_train = indices[:n_train]\n",
    "    indices_val = indices[n_train:]\n",
    "    \n",
    "    dataset_train = torch.utils.data.Subset(dataset_full, indices_train)\n",
    "    dataset_val = torch.utils.data.Subset(dataset_full, indices_val)\n",
    "    \n",
    "    print(f\"üìä Train: {len(dataset_train)} | Val: {len(dataset_val)}\")\n",
    "    \n",
    "    # DataLoader - T·ªêI ∆ØU CHO GPU\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=so_worker,\n",
    "        pin_memory=pin_memory,  # ‚ö° GPU t·ªëi ∆∞u\n",
    "        persistent_workers=(so_worker > 0)  # Gi·ªØ workers s·ªëng ƒë·ªÉ t√°i s·ª≠ d·ª•ng\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset_val, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=so_worker,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=(so_worker > 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ DataLoader t·∫°o th√†nh c√¥ng\")\n",
    "    print(f\"üí° M·ªói epoch: {len(train_loader)} batches training | {len(val_loader)} batches validation\")\n",
    "else:\n",
    "    train_loader = None\n",
    "    val_loader = None\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o DataLoader\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea5b4d",
   "metadata": {},
   "source": [
    "## 4) H√†m ƒë√°nh gi√° v√† l∆∞u ·∫£nh minh ho·∫°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7496e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== H√ÄM ƒê√ÅNH GI√Å & MINH H·ªåA ==================\n",
    "\n",
    "def psnr_tensor(x: torch.Tensor, y: torch.Tensor, max_pixel: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"T√≠nh PSNR gi·ªØa 2 tensor (gi·∫£ s·ª≠ gi√° tr·ªã trong [0,1]).\"\"\"\n",
    "    mse = torch.mean((x - y) ** 2)\n",
    "    if mse == 0:\n",
    "        return torch.tensor(float('inf'), device=x.device)\n",
    "    psnr = 20 * torch.log10(torch.tensor(max_pixel, device=x.device) / torch.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def mse_tensor(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"T√≠nh MSE gi·ªØa 2 tensor.\"\"\"\n",
    "    return torch.mean((x - y) ** 2)\n",
    "\n",
    "def them_nhieu(anh: torch.Tensor, loai: str = \"normal\", do_manh: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"Th√™m nhi·ªÖu v√†o ·∫£nh.\"\"\"\n",
    "    if loai == \"normal\":\n",
    "        nhieu = torch.randn_like(anh) * do_manh\n",
    "    elif loai == \"bernoulli\":\n",
    "        nhieu = torch.bernoulli(torch.full_like(anh, do_manh)) - 0.5\n",
    "    elif loai == \"poisson\":\n",
    "        nhieu = (torch.poisson(anh * 255.0 * do_manh) - anh * 255.0 * do_manh) / (255.0 * do_manh)\n",
    "    else:\n",
    "        nhieu = torch.zeros_like(anh)\n",
    "    return torch.clamp(anh + nhieu, 0, 1)\n",
    "\n",
    "def danh_gia(mo_hinh: nn.Module, loader: DataLoader, device: torch.device, \n",
    "             loai_nhieu: str, do_manh_nhieu: float) -> tuple:\n",
    "    \"\"\"ƒê√°nh gi√° m√¥ h√¨nh v√† tr·∫£ v·ªÅ PSNR, MSE, loss.\"\"\"\n",
    "    mo_hinh.eval()\n",
    "    psnr_vals, mse_vals, loss_vals = [], [], []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for anh_sach, _ in loader:\n",
    "            anh_sach = anh_sach.to(device)\n",
    "            anh_nhieu = them_nhieu(anh_sach, loai_nhieu, do_manh_nhieu)\n",
    "            \n",
    "            dau_ra = mo_hinh(anh_nhieu)\n",
    "            \n",
    "            psnr_vals.append(psnr_tensor(dau_ra, anh_sach).item())\n",
    "            mse_vals.append(mse_tensor(dau_ra, anh_sach).item())\n",
    "            loss_vals.append(loss_fn(dau_ra, anh_sach).item())\n",
    "    \n",
    "    return np.mean(psnr_vals), np.mean(mse_vals), np.mean(loss_vals)\n",
    "\n",
    "def luu_minh_hoa(mo_hinh: nn.Module, anh_mau: torch.Tensor, device: torch.device,\n",
    "                loai_nhieu: str, do_manh_nhieu: float, thu_muc: Path, tag: str = \"\"):\n",
    "    \"\"\"L∆∞u ·∫£nh minh ho·∫° denoising.\"\"\"\n",
    "    mo_hinh.eval()\n",
    "    with torch.no_grad():\n",
    "        anh_sach = anh_mau.to(device)\n",
    "        anh_nhieu = them_nhieu(anh_sach, loai_nhieu, do_manh_nhieu)\n",
    "        anh_khuc_phuc = mo_hinh(anh_nhieu)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    for idx, (anh, tieu_de) in enumerate([\n",
    "        (anh_nhieu[0], f\"Nhi·ªÖu ({loai_nhieu})\"),\n",
    "        (anh_khuc_phuc[0], \"Kh√¥i ph·ª•c\"),\n",
    "        (anh_sach[0], \"G·ªëc\")\n",
    "    ]):\n",
    "        axes[idx].imshow(anh.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[idx].set_title(tieu_de)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ten_file = f\"viz_{tag}.png\" if tag else \"viz.png\"\n",
    "    duong_dan_anh = thu_muc / ten_file\n",
    "    plt.savefig(duong_dan_anh, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cb076",
   "metadata": {},
   "source": [
    "## 5) T·∫£i d·ªØ li·ªáu (ImageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f48f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== KI·ªÇM TRA DATALOADER (ƒê√É T·ªêI ∆ØU GPU ·ªû CELL TR∆Ø·ªöC) ====================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ KI·ªÇM TRA DATALOADER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    print(f\"‚úÖ Train DataLoader: {len(train_loader)} batches\")\n",
    "    print(f\"‚úÖ Val DataLoader: {len(val_loader)} batches\")\n",
    "    print(f\"‚úÖ Batch size: {batch_size}\")\n",
    "    print(f\"‚úÖ Num workers: {so_worker}\")\n",
    "    print(f\"‚úÖ Pin memory: {pin_memory} (GPU t·ªëi ∆∞u)\")\n",
    "    print(f\"‚úÖ Non-blocking: True (GPU transfer t·ªëi ∆∞u)\")\n",
    "    print(\"\\nüí° DataLoader ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u GPU ·ªü cell tr∆∞·ªõc - KH√îNG T·∫†O L·∫†I!\")\n",
    "else:\n",
    "    print(\"‚ùå DataLoader ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o! Ch·∫°y l·∫°i cell tr∆∞·ªõc.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27650ad1",
   "metadata": {},
   "source": [
    "## 6) Hu·∫•n luy·ªán m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== HU·∫§N LUY·ªÜN AUTOENCODER (T·ªêI ∆ØU GPU + MULTI-GPU) ==================\n",
    "\n",
    "# Kh·ªüi t·∫°o optimizer + loss\n",
    "bo_toi_uu = optim.Adam(mo_hinh.parameters(), lr=learning_rate)\n",
    "\n",
    "if ten_loss == \"mse\":\n",
    "    ham_loss = nn.MSELoss()\n",
    "elif ten_loss == \"l1\":\n",
    "    ham_loss = nn.L1Loss()\n",
    "else:\n",
    "    raise ValueError(\"‚ùå ten_loss ph·∫£i l√† 'mse' ho·∫∑c 'l1'\")\n",
    "\n",
    "best_psnr = -1.0\n",
    "best_anh_mau = None\n",
    "lich_su = {\"loss_train\": [], \"psnr_val\": [], \"mse_val\": []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN AUTOENCODER\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"üéÆ Training v·ªõi {n_gpus} GPUs: {[torch.cuda.get_device_name(i) for i in range(n_gpus)]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(1, so_epoch + 1):\n",
    "    epoch_start = time.time()\n",
    "    mo_hinh.train()\n",
    "    tong_loss = 0.0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch}/{so_epoch}\", leave=False) as pbar:\n",
    "        for anh_sach, _ in pbar:\n",
    "            anh_sach = anh_sach.to(thiet_bi, non_blocking=True)  # Non-blocking GPU transfer\n",
    "            \n",
    "            # Input: ·∫£nh nhi·ªÖu\n",
    "            anh_nhieu = them_nhieu(anh_sach, loai_nhieu, do_manh_nhieu).to(thiet_bi, non_blocking=True)\n",
    "            \n",
    "            # Target\n",
    "            if kieu_muc_tieu == \"clean\":\n",
    "                muc_tieu = anh_sach\n",
    "            elif kieu_muc_tieu == \"noisy\":\n",
    "                muc_tieu = them_nhieu(anh_sach, loai_nhieu, do_manh_nhieu).to(thiet_bi, non_blocking=True)\n",
    "            else:\n",
    "                raise ValueError(\"‚ùå kieu_muc_tieu ph·∫£i l√† 'clean' ho·∫∑c 'noisy'\")\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    anh_tai_tao = mo_hinh(anh_nhieu)\n",
    "                    loss = ham_loss(anh_tai_tao, muc_tieu)\n",
    "                \n",
    "                # Backward with scaling\n",
    "                bo_toi_uu.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(bo_toi_uu)\n",
    "                torch.nn.utils.clip_grad_norm_(mo_hinh.parameters(), 1.0)\n",
    "                scaler.step(bo_toi_uu)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training - DataParallel t·ª± ƒë·ªông ph√¢n chia batch l√™n c√°c GPU\n",
    "                anh_tai_tao = mo_hinh(anh_nhieu)\n",
    "                loss = ham_loss(anh_tai_tao, muc_tieu)\n",
    "                \n",
    "                bo_toi_uu.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(mo_hinh.parameters(), 1.0)\n",
    "                bo_toi_uu.step()\n",
    "            \n",
    "            tong_loss += loss.item() * anh_sach.size(0)\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    loss_train = tong_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validate\n",
    "    psnr_val, mse_val, _ = danh_gia(mo_hinh, val_loader, thiet_bi, loai_nhieu, do_manh_nhieu)\n",
    "    \n",
    "    lich_su[\"loss_train\"].append(loss_train)\n",
    "    lich_su[\"psnr_val\"].append(psnr_val)\n",
    "    lich_su[\"mse_val\"].append(mse_val)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"[Epoch {epoch:02d}] ({epoch_time:.1f}s) \"\n",
    "          f\"Loss: {loss_train:.6f} | \"\n",
    "          f\"PSNR: {psnr_val:.2f}dB | \"\n",
    "          f\"MSE: {mse_val:.6f}\")\n",
    "    \n",
    "    # Ch·ªâ l∆∞u khi ƒë·∫°t PSNR t·ªët nh·∫•t\n",
    "    if psnr_val > best_psnr:\n",
    "        best_psnr = psnr_val\n",
    "        best_anh_mau, _ = next(iter(val_loader))\n",
    "        \n",
    "        # L∆∞u model - n·∫øu d√πng DataParallel, l∆∞u module.state_dict()\n",
    "        if use_multi_gpu:\n",
    "            torch.save(mo_hinh.module.state_dict(), duong_dan_checkpoint_ae)\n",
    "        else:\n",
    "            torch.save(mo_hinh.state_dict(), duong_dan_checkpoint_ae)\n",
    "        \n",
    "        print(f\"  ‚úÖ PSNR t·ªët nh·∫•t m·ªõi: {best_psnr:.4f}dB - Checkpoint l∆∞u th√†nh c√¥ng!\")\n",
    "\n",
    "total_time = time.time() - time_start\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH H·ªåC AUTOENCODER\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best PSNR: {best_psnr:.4f}\")\n",
    "print(f\"Th·ªùi gian: {total_time/60:.1f} ph√∫t ({total_time/3600:.2f} gi·ªù)\")\n",
    "print(f\"Thi·∫øt b·ªã: {thiet_bi} {'(GPU)' if use_cuda else '(CPU)'}\")\n",
    "if use_multi_gpu:\n",
    "    print(f\"üéÆ ƒê√£ s·ª≠ d·ª•ng {n_gpus} GPUs ƒë·ªìng th·ªùi!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# L∆∞u ·∫£nh minh ho·∫° CH·ªà T·ª™ MODEL T·ªêT NH·∫§T\n",
    "if best_anh_mau is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üñºÔ∏è L∆ØU·∫¢NH MINH H·ªåA T·ª™ MODEL T·ªêT NH·∫§T\")\n",
    "    print(\"=\"*60)\n",
    "    luu_minh_hoa(mo_hinh, best_anh_mau, thiet_bi, loai_nhieu, do_manh_nhieu, thu_muc_ket_qua, tag=\"best_autoencoder\")\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u ·∫£nh ch·∫•t l∆∞·ª£ng cao nh·∫•t (PSNR={best_psnr:.4f}dB)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Clear cache\n",
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nüíæ GPU cache ƒë√£ ƒë∆∞·ª£c x√≥a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664040ff",
   "metadata": {},
   "source": [
    "## 7) V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff730b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(lich_su[\"loss_train\"])\n",
    "plt.title(\"Loss hu·∫•n luy·ªán (Train Loss)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lich_su[\"psnr_val\"])\n",
    "plt.title(\"PSNR validation (dB)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"PSNR (dB)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lich_su[\"mse_val\"])\n",
    "plt.title(\"MSE validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc6cb8",
   "metadata": {},
   "source": [
    "## 8) (Tu·ª≥ ch·ªçn) Load l·∫°i checkpoint t·ªët nh·∫•t v√† l∆∞u ·∫£nh minh ho·∫°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c660bab",
   "metadata": {},
   "source": [
    "## 9) Image Denoising v·ªõi GAN (Generator + Discriminator)\n",
    "\n",
    "GAN Architecture:\n",
    "- **Generator**: M·∫°ng sinh ·∫£nh s·∫°ch t·ª´ ·∫£nh nhi·ªÖu\n",
    "- **Discriminator**: Ph√¢n bi·ªát ·∫£nh s·∫°ch th·ª±c vs ·∫£nh sinh ra\n",
    "- **Loss**: Adversarial Loss + Reconstruction Loss (L1/MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== KI·∫æN TR√öC GAN ====================\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator t∆∞∆°ng t·ª± Autoencoder nh∆∞ng output activation kh√°c.\"\"\"\n",
    "    def __init__(self, so_kenh_vao: int = 3, kenh_co_so: int = 32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(so_kenh_vao, kenh_co_so, 3, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so), nn.ReLU(True),\n",
    "            nn.Conv2d(kenh_co_so, kenh_co_so*2, 3, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*2), nn.ReLU(True),\n",
    "            nn.Conv2d(kenh_co_so*2, kenh_co_so*4, 3, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*4), nn.ReLU(True),\n",
    "            nn.Conv2d(kenh_co_so*4, kenh_co_so*8, 3, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*8), nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(kenh_co_so*8, kenh_co_so*4, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(kenh_co_so*4, kenh_co_so*2, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(kenh_co_so*2, kenh_co_so, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(kenh_co_so, so_kenh_vao, 4, stride=2, padding=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator ph√¢n bi·ªát ·∫£nh th·ª±c vs gi·∫£ b·∫±ng CNN.\"\"\"\n",
    "    def __init__(self, so_kenh_vao: int = 3, kenh_co_so: int = 32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(so_kenh_vao, kenh_co_so, 4, stride=2, padding=1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(kenh_co_so, kenh_co_so*2, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(kenh_co_so*2, kenh_co_so*4, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(kenh_co_so*4, kenh_co_so*8, 4, stride=2, padding=1), nn.BatchNorm2d(kenh_co_so*8), nn.LeakyReLU(0.2, True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(kenh_co_so*8, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o GAN\n",
    "generator_gan = Generator(so_kenh_vao=3, kenh_co_so=so_kenh_co_so).to(thiet_bi)\n",
    "discriminator_gan = Discriminator(so_kenh_vao=3, kenh_co_so=so_kenh_co_so).to(thiet_bi)\n",
    "\n",
    "# ============ WRAP V·ªöI DataParallel N·∫æU C√ì NHI·ªÄU GPU ============\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\nüöÄ Wrapping GAN v·ªõi DataParallel ({n_gpus} GPUs)\")\n",
    "    generator_gan = nn.DataParallel(generator_gan)\n",
    "    discriminator_gan = nn.DataParallel(discriminator_gan)\n",
    "    print(f\"‚úÖ Generator device_ids: {generator_gan.device_ids}\")\n",
    "    print(f\"‚úÖ Discriminator device_ids: {discriminator_gan.device_ids}\")\n",
    "\n",
    "print(\"‚úÖ Generator:\")\n",
    "print(generator_gan)\n",
    "print(\"\\n‚úÖ Discriminator:\")\n",
    "print(discriminator_gan)\n",
    "\n",
    "if use_multi_gpu:\n",
    "    print(f\"\\nüéÆ Multi-GPU GAN Training ENABLED: {n_gpus} GPUs ƒë·ªìng th·ªùi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b85612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== H√ÄM H·ªñ TR·ª¢ GAN (T·ªêI ∆ØU GPU + MULTI-GPU) ====================\n",
    "\n",
    "def train_gan(generator: nn.Module, discriminator: nn.Module, \n",
    "              train_loader: DataLoader, val_loader: DataLoader,\n",
    "              epochs: int, device: torch.device, lambda_recon: float = 100.0) -> tuple:\n",
    "    \"\"\"Hu·∫•n luy·ªán GAN cho denoising v·ªõi Adversarial Loss + Reconstruction Loss.\n",
    "    \n",
    "    T·ªëi ∆∞u h√≥a cho GPU:\n",
    "    - Mixed Precision Training (FP16)\n",
    "    - Multi-GPU support v·ªõi DataParallel\n",
    "    - GPU memory efficient training\n",
    "    \"\"\"\n",
    "    \n",
    "    optim_g = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    optim_d = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    loss_gan = nn.BCEWithLogitsLoss()\n",
    "    loss_recon = nn.L1Loss()\n",
    "    \n",
    "    history = {'g_loss': [], 'd_loss': [], 'psnr_val': [], 'mse_val': [], 'best_psnr': 0}\n",
    "    best_psnr = 0.0\n",
    "    best_checkpoint_gan = None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN GAN\")\n",
    "    if use_multi_gpu:\n",
    "        print(f\"üéÆ Training v·ªõi {n_gpus} GPUs: {[torch.cuda.get_device_name(i) for i in range(n_gpus)]}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Start timing\n",
    "    time_start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        g_loss_epoch, d_loss_epoch = 0.0, 0.0\n",
    "        so_batches = 0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False) as pbar:\n",
    "            for batch_idx, (img_nhieu, _) in enumerate(pbar):\n",
    "                img_nhieu = img_nhieu.to(device, non_blocking=True)  # Non-blocking transfer\n",
    "                img_sach = img_nhieu.clone()\n",
    "                B = img_nhieu.size(0)\n",
    "                \n",
    "                # ============ C·∫¨P NH·∫¨T DISCRIMINATOR ============\n",
    "                optim_d.zero_grad()\n",
    "                \n",
    "                # S·ª≠ d·ª•ng autocast n·∫øu mixed precision ƒë∆∞·ª£c b·∫≠t\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        # Loss discriminator tr√™n ·∫£nh th·∫≠t\n",
    "                        label_thuc = torch.ones(B, 1, device=device)\n",
    "                        output_thuc = discriminator(img_sach)\n",
    "                        d_loss_thuc = loss_gan(output_thuc, label_thuc)\n",
    "                        \n",
    "                        # Loss discriminator tr√™n ·∫£nh gi·∫£\n",
    "                        with torch.no_grad():\n",
    "                            img_tao = generator(img_nhieu)\n",
    "                        label_gia = torch.zeros(B, 1, device=device)\n",
    "                        output_gia = discriminator(img_tao.detach())\n",
    "                        d_loss_gia = loss_gan(output_gia, label_gia)\n",
    "                        \n",
    "                        d_loss = d_loss_thuc + d_loss_gia\n",
    "                    \n",
    "                    scaler.scale(d_loss).backward()\n",
    "                    scaler.unscale_(optim_d)\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                    scaler.step(optim_d)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Loss discriminator tr√™n ·∫£nh th·∫≠t - DataParallel t·ª± ƒë·ªông ph√¢n chia\n",
    "                    label_thuc = torch.ones(B, 1, device=device)\n",
    "                    output_thuc = discriminator(img_sach)\n",
    "                    d_loss_thuc = loss_gan(output_thuc, label_thuc)\n",
    "                    \n",
    "                    # Loss discriminator tr√™n ·∫£nh gi·∫£\n",
    "                    with torch.no_grad():\n",
    "                        img_tao = generator(img_nhieu)\n",
    "                    label_gia = torch.zeros(B, 1, device=device)\n",
    "                    output_gia = discriminator(img_tao.detach())\n",
    "                    d_loss_gia = loss_gan(output_gia, label_gia)\n",
    "                    \n",
    "                    d_loss = d_loss_thuc + d_loss_gia\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                    optim_d.step()\n",
    "                \n",
    "                # ============ C·∫¨P NH·∫¨T GENERATOR ============\n",
    "                optim_g.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        img_tao = generator(img_nhieu)\n",
    "                        \n",
    "                        # Adversarial loss\n",
    "                        output_gia = discriminator(img_tao)\n",
    "                        g_loss_adv = loss_gan(output_gia, label_thuc)\n",
    "                        \n",
    "                        # Reconstruction loss\n",
    "                        g_loss_recon = loss_recon(img_tao, img_sach)\n",
    "                        \n",
    "                        # T·ªïng loss\n",
    "                        g_loss = g_loss_adv + lambda_recon * g_loss_recon\n",
    "                    \n",
    "                    scaler.scale(g_loss).backward()\n",
    "                    scaler.unscale_(optim_g)\n",
    "                    torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                    scaler.step(optim_g)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    img_tao = generator(img_nhieu)\n",
    "                    \n",
    "                    # Adversarial loss\n",
    "                    output_gia = discriminator(img_tao)\n",
    "                    g_loss_adv = loss_gan(output_gia, label_thuc)\n",
    "                    \n",
    "                    # Reconstruction loss\n",
    "                    g_loss_recon = loss_recon(img_tao, img_sach)\n",
    "                    \n",
    "                    # T·ªïng loss\n",
    "                    g_loss = g_loss_adv + lambda_recon * g_loss_recon\n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                    optim_g.step()\n",
    "                \n",
    "                g_loss_epoch += g_loss.item()\n",
    "                d_loss_epoch += d_loss.item()\n",
    "                so_batches += 1\n",
    "                \n",
    "                pbar.set_postfix({'G': f'{g_loss.item():.3f}', 'D': f'{d_loss.item():.3f}'})\n",
    "        \n",
    "        # Trung b√¨nh loss\n",
    "        g_loss_epoch /= so_batches\n",
    "        d_loss_epoch /= so_batches\n",
    "        history['g_loss'].append(g_loss_epoch)\n",
    "        history['d_loss'].append(d_loss_epoch)\n",
    "        \n",
    "        # ============ ƒê√ÅNH GI√Å VALIDATION ============\n",
    "        generator.eval()\n",
    "        psnr_vals, mse_vals = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img_nhieu, _ in val_loader:\n",
    "                img_nhieu = img_nhieu.to(device, non_blocking=True)\n",
    "                img_sach = img_nhieu.clone()\n",
    "                img_tao = generator(img_nhieu)\n",
    "                \n",
    "                psnr_vals.append(psnr_tensor(img_tao, img_sach).item())\n",
    "                mse_vals.append(mse_tensor(img_tao, img_sach).item())\n",
    "        \n",
    "        psnr_mean = np.mean(psnr_vals)\n",
    "        mse_mean = np.mean(mse_vals)\n",
    "        history['psnr_val'].append(psnr_mean)\n",
    "        history['mse_val'].append(mse_mean)\n",
    "        \n",
    "        # T√≠nh th·ªùi gian epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # L∆∞u checkpoint t·ªët nh·∫•t\n",
    "        if psnr_mean > best_psnr:\n",
    "            best_psnr = psnr_mean\n",
    "            \n",
    "            # L∆∞u state_dict - n·∫øu d√πng DataParallel, l∆∞u module.state_dict()\n",
    "            if use_multi_gpu:\n",
    "                best_checkpoint_gan = {\n",
    "                    'generator': generator.module.state_dict(),\n",
    "                    'discriminator': discriminator.module.state_dict(),\n",
    "                    'epoch': epoch\n",
    "                }\n",
    "                torch.save(generator.module.state_dict(), duong_dan_checkpoint_gan_g)\n",
    "                torch.save(discriminator.module.state_dict(), duong_dan_checkpoint_gan_d)\n",
    "            else:\n",
    "                best_checkpoint_gan = {\n",
    "                    'generator': generator.state_dict(),\n",
    "                    'discriminator': discriminator.state_dict(),\n",
    "                    'epoch': epoch\n",
    "                }\n",
    "                torch.save(generator.state_dict(), duong_dan_checkpoint_gan_g)\n",
    "                torch.save(discriminator.state_dict(), duong_dan_checkpoint_gan_d)\n",
    "            \n",
    "            history['best_psnr'] = best_psnr\n",
    "            \n",
    "            print(f\"‚úÖ Epoch {epoch+1} ({epoch_time:.1f}s): G={g_loss_epoch:.4f}, D={d_loss_epoch:.4f}, \"\n",
    "                  f\"PSNR={psnr_mean:.3f}dB [SAVED]\")\n",
    "        else:\n",
    "            print(f\"   Epoch {epoch+1} ({epoch_time:.1f}s): G={g_loss_epoch:.4f}, D={d_loss_epoch:.4f}, \"\n",
    "                  f\"PSNR={psnr_mean:.3f}dB\")\n",
    "    \n",
    "    if best_checkpoint_gan:\n",
    "        if use_multi_gpu:\n",
    "            generator.module.load_state_dict(best_checkpoint_gan['generator'])\n",
    "            discriminator.module.load_state_dict(best_checkpoint_gan['discriminator'])\n",
    "        else:\n",
    "            generator.load_state_dict(best_checkpoint_gan['generator'])\n",
    "            discriminator.load_state_dict(best_checkpoint_gan['discriminator'])\n",
    "    \n",
    "    total_time = time.time() - time_start\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üéâ HO√ÄN TH√ÄNH H·ªåC GAN\")\n",
    "    print(f\"   Best PSNR: {best_psnr:.4f}\")\n",
    "    print(f\"   Th·ªùi gian: {total_time/60:.1f} ph√∫t ({total_time/3600:.2f} gi·ªù)\")\n",
    "    print(f\"   Thi·∫øt b·ªã: {device}\")\n",
    "    if use_multi_gpu:\n",
    "        print(f\"   üéÆ ƒê√£ s·ª≠ d·ª•ng {n_gpus} GPUs ƒë·ªìng th·ªùi!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history, generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca176517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CH·∫†Y HU·∫§N LUY·ªÜN GAN ====================\n",
    "\n",
    "# G·ªçi h√†m train_gan ƒë·ªÉ hu·∫•n luy·ªán\n",
    "history_gan, generator_gan, discriminator_gan = train_gan(\n",
    "    generator_gan, \n",
    "    discriminator_gan, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epochs=so_epoch, \n",
    "    device=thiet_bi, \n",
    "    lambda_recon=100.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae75050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ƒê·ªí TH·ªä SO S√ÅNH H·ªåC HU·∫§N LUY·ªÜN ====================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('So s√°nh qu√° tr√¨nh hu·∫•n luy·ªán: Autoencoder vs GAN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# G Loss\n",
    "axes[0, 0].plot(history_gan['g_loss'], 'b-', linewidth=2, label='Generator Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Generator Loss qua c√°c epoch')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# D Loss\n",
    "axes[0, 1].plot(history_gan['d_loss'], 'r-', linewidth=2, label='Discriminator Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Discriminator Loss qua c√°c epoch')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# PSNR Comparison\n",
    "axes[1, 0].plot(lich_su['psnr_val'], 'g-o', linewidth=2, label='Autoencoder', markersize=5)\n",
    "axes[1, 0].plot(history_gan['psnr_val'], 'b-s', linewidth=2, label='GAN', markersize=5)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('PSNR (dB)')\n",
    "axes[1, 0].set_title('PSNR tr√™n validation set')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# MSE Comparison\n",
    "axes[1, 1].plot(lich_su['mse_val'], 'g-o', linewidth=2, label='Autoencoder', markersize=5)\n",
    "axes[1, 1].plot(history_gan['mse_val'], 'b-s', linewidth=2, label='GAN', markersize=5)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title('MSE tr√™n validation set')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(thu_muc_ket_qua / 'comparison_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh hu·∫•n luy·ªán\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33297664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== L∆ØU ·∫¢NH MINH H·ªåA T·ª™ MODEL GAN T·ªêT NH·∫§T ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üñºÔ∏è L∆ØU ·∫¢NH MINH H·ªåA T·ª™ GAN T·ªêT NH·∫§T\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# L·∫•y 1 batch t·ª´ validation set ƒë·ªÉ minh ho·∫°\n",
    "best_gan_sample, _ = next(iter(val_loader))\n",
    "luu_minh_hoa(generator_gan, best_gan_sample, thiet_bi, loai_nhieu, do_manh_nhieu, thu_muc_ket_qua, tag=\"best_gan_generator\")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u ·∫£nh GAN ch·∫•t l∆∞·ª£ng cao nh·∫•t (PSNR={history_gan['best_psnr']:.4f}dB)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear cache\n",
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nüíæ GPU cache ƒë√£ ƒë∆∞·ª£c x√≥a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57749c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== B·∫¢NG SO S√ÅNH CHUY√äN S√ÇU√ÇU ====================\n",
    "\n",
    "# T√≠nh ƒë·ªô ƒëo tr√™n to√†n b·ªô validation set\n",
    "def evaluate_model(model: nn.Module, val_loader: DataLoader, device: torch.device) -> dict:\n",
    "    \"\"\"ƒê√°nh gi√° chi ti·∫øt m√¥ h√¨nh tr√™n validation set.\"\"\"\n",
    "    model.eval()\n",
    "    psnr_vals, mse_vals, l1_vals = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_clean, _ in val_loader:\n",
    "            img_clean = img_clean.to(device)\n",
    "            img_noisy = them_nhieu(img_clean, loai_nhieu, do_manh_nhieu).to(device, non_blocking=True)\n",
    "            img_denoised = model(img_noisy)\n",
    "            \n",
    "            psnr_vals.append(psnr_tensor(img_denoised, img_clean).item())\n",
    "            mse_vals.append(mse_tensor(img_denoised, img_clean).item())\n",
    "            l1_vals.append(torch.nn.functional.l1_loss(img_denoised, img_clean).item())\n",
    "    \n",
    "    return {\n",
    "        'psnr_mean': np.mean(psnr_vals),\n",
    "        'psnr_std': np.std(psnr_vals),\n",
    "        'mse_mean': np.mean(mse_vals),\n",
    "        'mse_std': np.std(mse_vals),\n",
    "        'l1_mean': np.mean(l1_vals),\n",
    "        'l1_std': np.std(l1_vals),\n",
    "    }\n",
    "\n",
    "# ƒê√°nh gi√° Autoencoder\n",
    "metrics_ae = evaluate_model(mo_hinh, val_loader, thiet_bi)\n",
    "\n",
    "# ƒê√°nh gi√° GAN\n",
    "metrics_gan = evaluate_model(generator_gan, val_loader, thiet_bi)\n",
    "\n",
    "# T·∫°o b·∫£ng so s√°nh\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*20 + \"üìä B·∫¢NG SO S√ÅNH AUTOENCODER vs GAN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Ch·ªâ s·ªë':25} {'Autoencoder':^25} {'GAN':^25}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'PSNR (dB)':25} {metrics_ae['psnr_mean']:>10.4f} ¬± {metrics_ae['psnr_std']:>6.4f}   |   \"\n",
    "      f\"{metrics_gan['psnr_mean']:>10.4f} ¬± {metrics_gan['psnr_std']:>6.4f}\")\n",
    "print(f\"{'MSE':25} {metrics_ae['mse_mean']:>10.6f} ¬± {metrics_ae['mse_std']:>6.6f}   |   \"\n",
    "      f\"{metrics_gan['mse_mean']:>10.6f} ¬± {metrics_gan['mse_std']:>6.6f}\")\n",
    "print(f\"{'L1 Loss':25} {metrics_ae['l1_mean']:>10.6f} ¬± {metrics_ae['l1_std']:>6.6f}   |   \"\n",
    "      f\"{metrics_gan['l1_mean']:>10.6f} ¬± {metrics_gan['l1_std']:>6.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# X√°c ƒë·ªãnh m√¥ h√¨nh t·ªët h∆°n\n",
    "if metrics_gan['psnr_mean'] > metrics_ae['psnr_mean']:\n",
    "    print(f\"üèÜ K·∫æT LU·∫¨N: GAN v∆∞·ª£t tr·ªôi h∆°n v·ªõi PSNR cao h∆°n {metrics_gan['psnr_mean'] - metrics_ae['psnr_mean']:.4f} dB\")\n",
    "else:\n",
    "    print(f\"üèÜ K·∫æT LU·∫¨N: Autoencoder v∆∞·ª£t tr·ªôi h∆°n v·ªõi PSNR cao h∆°n {metrics_ae['psnr_mean'] - metrics_gan['psnr_mean']:.4f} dB\")\n",
    "\n",
    "print(f\"\\nüìà T·∫•t c·∫£ ch·ªâ s·ªë chi ti·∫øt:\")\n",
    "print(f\"   Autoencoder: {metrics_ae}\")\n",
    "print(f\"   GAN:         {metrics_gan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== H√åNH ·∫¢NH SO S√ÅNH TR·ª∞C QUAN ====================\n",
    "\n",
    "def visualize_comparison(val_loader: DataLoader, autoencoder: nn.Module, \n",
    "                        generator: nn.Module, device: torch.device, n_samples: int = 4):\n",
    "    \"\"\"So s√°nh tr·ª±c quan k·∫øt qu·∫£ Autoencoder vs GAN.\"\"\"\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    generator.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "    fig.suptitle('So s√°nh k·∫øt qu·∫£ Autoencoder vs GAN tr√™n validation set', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_clean, _ in val_loader:\n",
    "            if sample_count >= n_samples:\n",
    "                break\n",
    "            \n",
    "            img_clean = img_clean.to(device)\n",
    "            img_noisy = them_nhieu(img_clean, loai_nhieu, do_manh_nhieu).to(device, non_blocking=True)\n",
    "            \n",
    "            for i in range(img_clean.size(0)):\n",
    "                if sample_count >= n_samples:\n",
    "                    break\n",
    "                \n",
    "                # Forward pass\n",
    "                ae_output = autoencoder(img_noisy[i:i+1])\n",
    "                gan_output = generator(img_noisy[i:i+1])\n",
    "                \n",
    "                # T√≠nh PSNR\n",
    "                psnr_ae = psnr_tensor(ae_output, img_clean[i:i+1]).item()\n",
    "                psnr_gan = psnr_tensor(gan_output, img_clean[i:i+1]).item()\n",
    "                \n",
    "                # Chuy·ªÉn sang NumPy ƒë·ªÉ hi·ªÉn th·ªã\n",
    "                def to_display(tensor):\n",
    "                    return tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                \n",
    "                img_noisy_np = to_display(img_noisy[i:i+1])\n",
    "                img_clean_np = to_display(img_clean[i:i+1])\n",
    "                ae_np = to_display(ae_output)\n",
    "                gan_np = to_display(gan_output)\n",
    "                \n",
    "                # H√†ng (i)\n",
    "                row = sample_count\n",
    "                \n",
    "                # C·ªôt 0: ·∫¢nh nhi·ªÖu\n",
    "                axes[row, 0].imshow(img_noisy_np)\n",
    "                axes[row, 0].set_title(f'·∫¢nh nhi·ªÖu\\n({loai_nhieu}, {do_manh_nhieu:.2f})', fontsize=10)\n",
    "                axes[row, 0].axis('off')\n",
    "                \n",
    "                # C·ªôt 1: ·∫¢nh g·ªëc\n",
    "                axes[row, 1].imshow(img_clean_np)\n",
    "                axes[row, 1].set_title('·∫¢nh g·ªëc (Ground Truth)', fontsize=10)\n",
    "                axes[row, 1].axis('off')\n",
    "                \n",
    "                # C·ªôt 2: K·∫øt qu·∫£ Autoencoder\n",
    "                axes[row, 2].imshow(ae_np)\n",
    "                axes[row, 2].set_title(f'Autoencoder\\nPSNR={psnr_ae:.2f}dB', \n",
    "                                      fontsize=10, fontweight='bold', color='green')\n",
    "                axes[row, 2].axis('off')\n",
    "                \n",
    "                # C·ªôt 3: K·∫øt qu·∫£ GAN\n",
    "                axes[row, 3].imshow(gan_np)\n",
    "                axes[row, 3].set_title(f'GAN\\nPSNR={psnr_gan:.2f}dB', \n",
    "                                      fontsize=10, fontweight='bold', color='blue')\n",
    "                axes[row, 3].axis('off')\n",
    "                \n",
    "                sample_count += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(thu_muc_ket_qua / 'ae_vs_gan_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u h√¨nh ·∫£nh so s√°nh (ƒë√£ so s√°nh {sample_count} m·∫´u)\")\n",
    "\n",
    "# Hi·ªÉn th·ªã so s√°nh\n",
    "visualize_comparison(val_loader, mo_hinh, generator_gan, thiet_bi, n_samples=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
