{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2086863a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:46.126239Z",
     "iopub.status.busy": "2025-12-21T03:46:46.125728Z",
     "iopub.status.idle": "2025-12-21T03:46:46.132068Z",
     "shell.execute_reply": "2025-12-21T03:46:46.131369Z",
     "shell.execute_reply.started": "2025-12-21T03:46:46.126211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78298f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:46.133606Z",
     "iopub.status.busy": "2025-12-21T03:46:46.133355Z",
     "iopub.status.idle": "2025-12-21T03:46:46.193271Z",
     "shell.execute_reply": "2025-12-21T03:46:46.192713Z",
     "shell.execute_reply.started": "2025-12-21T03:46:46.133585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for dataset...\n",
      "‚úÖ Found dataset: PennFudanPed\n",
      "üöÄ RUNNING ON KAGGLE\n",
      "Input dataset: /kaggle/input/pennfudanped/PennFudanPed\n",
      "Output directory: /kaggle/working\n",
      "\n",
      "‚úÖ Directories ready:\n",
      "   - PNGImages: /kaggle/input/pennfudanped/PennFudanPed/PNGImages\n",
      "     ‚îú‚îÄ 170 PNG files\n",
      "   - PedMasks: /kaggle/input/pennfudanped/PennFudanPed/PedMasks\n",
      "     ‚îú‚îÄ 170 Mask files\n",
      "   - crops64: /kaggle/working/crops64\n",
      "   - output: /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# ============ KAGGLE PATHS SETUP ============\n",
    "# üéØ CH·∫†Y TR√äN KAGGLE - T·ª∞ ƒê·ªòNG T√åM DATASET\n",
    "\n",
    "import pathlib\n",
    "\n",
    "print(\"\\nüîç Searching for dataset...\")\n",
    "\n",
    "# Tr√™n Kaggle, c√°c dataset ƒë∆∞·ª£c l∆∞u trong /kaggle/input/\n",
    "kaggle_input_dir = \"/kaggle/input/pennfudanped\"\n",
    "kaggle_output_dir = \"/kaggle/working\"\n",
    "\n",
    "if os.path.exists(kaggle_input_dir):\n",
    "    # T√¨m folder ch·ª©a PNGImages\n",
    "    root = None\n",
    "    for folder in os.listdir(kaggle_input_dir):\n",
    "        folder_path = os.path.join(kaggle_input_dir, folder)\n",
    "        png_path = os.path.join(folder_path, \"PNGImages\")\n",
    "        if os.path.exists(png_path):\n",
    "            root = folder_path\n",
    "            print(f\"‚úÖ Found dataset: {folder}\")\n",
    "            break\n",
    "    \n",
    "    if root is None:\n",
    "        raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y dataset ch·ª©a PNGImages trong {kaggle_input_dir}\")\n",
    "    \n",
    "    output_dir = kaggle_output_dir\n",
    "    print(f\"üöÄ RUNNING ON KAGGLE\")\n",
    "    print(f\"Input dataset: {root}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "else:\n",
    "    # Local fallback\n",
    "    root = pathlib.Path(r\"d:\\Master\\ComputerVision\\ComputerVisionCode\\PennFudanPed\")\n",
    "    output_dir = root\n",
    "    print(f\"üíª RUNNING LOCAL\")\n",
    "    print(f\"Dataset: {root}\")\n",
    "\n",
    "# Set up paths\n",
    "img_dir = os.path.join(root, \"PNGImages\")\n",
    "mask_dir = os.path.join(root, \"PedMasks\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(img_dir):\n",
    "    raise FileNotFoundError(f\"‚ùå PNGImages not found: {img_dir}\")\n",
    "if not os.path.exists(mask_dir):\n",
    "    raise FileNotFoundError(f\"‚ùå PedMasks not found: {mask_dir}\")\n",
    "\n",
    "# Create directories in writable location (NOT in read-only input folder)\n",
    "crop_dir = os.path.join(output_dir, \"crops64\")\n",
    "pos_dir = os.path.join(output_dir, \"crops64_pos\")\n",
    "neg_dir = os.path.join(output_dir, \"crops64_neg\")\n",
    "\n",
    "# Create output dirs\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "os.makedirs(pos_dir, exist_ok=True)\n",
    "os.makedirs(neg_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Directories ready:\")\n",
    "print(f\"   - PNGImages: {img_dir}\")\n",
    "print(f\"     ‚îú‚îÄ {len(glob.glob(os.path.join(img_dir, '*.png')))} PNG files\")\n",
    "print(f\"   - PedMasks: {mask_dir}\")\n",
    "print(f\"     ‚îú‚îÄ {len(glob.glob(os.path.join(mask_dir, '*.png')))} Mask files\")\n",
    "print(f\"   - crops64: {crop_dir}\")\n",
    "print(f\"   - output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4623d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:46.194462Z",
     "iopub.status.busy": "2025-12-21T03:46:46.194198Z",
     "iopub.status.idle": "2025-12-21T03:46:46.350408Z",
     "shell.execute_reply": "2025-12-21T03:46:46.349695Z",
     "shell.execute_reply.started": "2025-12-21T03:46:46.194440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ GPU SETUP - KAGGLE OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "1. ‚úÖ CUDA Available: True\n",
      "2. ‚úÖ PyTorch Version: 2.8.0+cu126\n",
      "\n",
      "3. GPU Count: 1\n",
      "4. GPU Name: Tesla P100-PCIE-16GB\n",
      "5. GPU Memory: 17.06 GB\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ TRAIN B·∫∞NG GPU: Tesla P100-PCIE-16GB\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============ GPU SETUP ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ GPU SETUP - KAGGLE OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1Ô∏è‚É£ Check CUDA\n",
    "print(f\"\\n1. ‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"2. ‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 2Ô∏è‚É£ Get GPU info\n",
    "    print(f\"\\n3. GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"4. GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"5. GPU Memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "    # 3Ô∏è‚É£ Force GPU usage\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "    # Optimize memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ TRAIN B·∫∞NG GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU NOT FOUND - Using CPU (SLOW)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864de896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:46.351728Z",
     "iopub.status.busy": "2025-12-21T03:46:46.351422Z",
     "iopub.status.idle": "2025-12-21T03:46:46.357806Z",
     "shell.execute_reply": "2025-12-21T03:46:46.357134Z",
     "shell.execute_reply.started": "2025-12-21T03:46:46.351702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ load_target() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============ LOAD TARGET FUNCTION ============\n",
    "def load_target(mask_p):\n",
    "    \"\"\"Extract bounding boxes and masks from annotation mask\"\"\"\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    obj_ids = np.unique(mask)[1:]  # Remove background\n",
    "    masks = (mask[..., None] == obj_ids).astype(np.uint8).transpose(2,0,1)\n",
    "    boxes = []\n",
    "    for m in masks:\n",
    "        pos = np.argwhere(m)\n",
    "        y1, x1 = pos.min(0)\n",
    "        y2, x2 = pos.max(0)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((len(boxes),), dtype=torch.int64)  # class=1 (person)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return boxes, labels, masks\n",
    "\n",
    "print(\"‚úÖ load_target() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e095eda9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:46.359883Z",
     "iopub.status.busy": "2025-12-21T03:46:46.359635Z",
     "iopub.status.idle": "2025-12-21T03:46:54.427835Z",
     "shell.execute_reply": "2025-12-21T03:46:54.427175Z",
     "shell.execute_reply.started": "2025-12-21T03:46:46.359863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì∏ CREATING 64x64 CROPS FROM DATASET (WITH AUGMENTATION)\n",
      "================================================================================\n",
      "‚úÖ Original crops found: ~126\n",
      "‚úÖ After 5x augmentation: 2115 images\n",
      "üìÅ Saved to: /kaggle/working/crops64\n"
     ]
    }
   ],
   "source": [
    "# ============ CREATE 64x64 CROPS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì∏ CREATING 64x64 CROPS FROM DATASET (WITH AUGMENTATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64,64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# ========== T·∫†NG CROPS V·ªöI AUGMENTATION ==========\n",
    "crop_count = 0\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    \n",
    "    # V·ªõi m·ªói ng∆∞·ªùi, t·∫°o 5 augmented crops\n",
    "    for person_idx, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        \n",
    "        # Version 1: Original crop\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v1.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 2: Rotated ¬±15¬∞\n",
    "        crop_rot = img.crop((x1, y1, x2, y2)).rotate(15, expand=False)\n",
    "        crop_rot = resize64(crop_rot)\n",
    "        crop_rot.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v2_rot.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 3: Rotated ‚àì15¬∞\n",
    "        crop_rot2 = img.crop((x1, y1, x2, y2)).rotate(-15, expand=False)\n",
    "        crop_rot2 = resize64(crop_rot2)\n",
    "        crop_rot2.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v3_rot.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 4: Flipped horizontally\n",
    "        crop_flip = img.crop((x1, y1, x2, y2)).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        crop_flip = resize64(crop_flip)\n",
    "        crop_flip.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v4_flip.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 5: Brightness adjusted\n",
    "        from PIL import ImageEnhance\n",
    "        crop_bright = img.crop((x1, y1, x2, y2))\n",
    "        enhancer = ImageEnhance.Brightness(crop_bright)\n",
    "        crop_bright = enhancer.enhance(1.2)  # 20% brighter\n",
    "        crop_bright = resize64(crop_bright)\n",
    "        crop_bright.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v5_bright.png\"))\n",
    "        crop_count += 1\n",
    "\n",
    "print(f\"‚úÖ Original crops found: ~126\")\n",
    "print(f\"‚úÖ After 5x augmentation: {crop_count} images\")\n",
    "print(f\"üìÅ Saved to: {crop_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7cec351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:46:54.428988Z",
     "iopub.status.busy": "2025-12-21T03:46:54.428746Z",
     "iopub.status.idle": "2025-12-21T03:47:01.026801Z",
     "shell.execute_reply": "2025-12-21T03:47:01.026070Z",
     "shell.execute_reply.started": "2025-12-21T03:46:54.428956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß CREATING BINARY CLASSIFICATION DATASET\n",
      "================================================================================\n",
      "‚úÖ Positive samples: 423 ·∫£nh ‚Üí /kaggle/working/crops64_pos\n",
      "‚úÖ Negative samples: 204 ·∫£nh ‚Üí /kaggle/working/crops64_neg\n",
      "üìä Ratio: 423/627 positive (67.5%)\n"
     ]
    }
   ],
   "source": [
    "# ============ CREATE POSITIVE/NEGATIVE SAMPLES ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß CREATING BINARY CLASSIFICATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# ========== POSITIVE SAMPLES (People) ==========\n",
    "pos_count = 0\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(pos_dir, f\"{base}_{i}.png\"))\n",
    "        pos_count += 1\n",
    "\n",
    "# ========== NEGATIVE SAMPLES (Background) ==========\n",
    "neg_count = 0\n",
    "np.random.seed(42)\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    img_h, img_w = img.size\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        w_crop, h_crop = 80, 80\n",
    "        x_rand = np.random.randint(0, max(img_w - w_crop, 1))\n",
    "        y_rand = np.random.randint(0, max(img_h - h_crop, 1))\n",
    "        \n",
    "        has_person = False\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            if not (x_rand + w_crop < x1 or x_rand > x2 or \n",
    "                    y_rand + h_crop < y1 or y_rand > y2):\n",
    "                has_person = True\n",
    "                break\n",
    "        \n",
    "        if not has_person:\n",
    "            crop = img.crop((x_rand, y_rand, x_rand + w_crop, y_rand + h_crop))\n",
    "            crop = crop.resize((64, 64))\n",
    "            crop.save(os.path.join(neg_dir, f\"{base}_neg_{attempt}.png\"))\n",
    "            neg_count += 1\n",
    "\n",
    "print(f\"‚úÖ Positive samples: {pos_count} ·∫£nh ‚Üí {pos_dir}\")\n",
    "print(f\"‚úÖ Negative samples: {neg_count} ·∫£nh ‚Üí {neg_dir}\")\n",
    "print(f\"üìä Ratio: {pos_count}/{pos_count+neg_count} positive ({100*pos_count/(pos_count+neg_count):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d3101c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:47:01.027949Z",
     "iopub.status.busy": "2025-12-21T03:47:01.027690Z",
     "iopub.status.idle": "2025-12-21T03:47:09.091819Z",
     "shell.execute_reply": "2025-12-21T03:47:09.091190Z",
     "shell.execute_reply.started": "2025-12-21T03:47:01.027926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ CNN (RESNET18) - BINARY CLASSIFICATION\n",
      "================================================================================\n",
      "üìä Dataset: 501 train + 126 val\n",
      "   Positive: 423\n",
      "   Negative: 204\n",
      "üñ•Ô∏è  Device: cuda\n",
      "\n",
      "Epoch  1/10: val_acc=0.341 | train_loss=7.0630\n",
      "Epoch  2/10: val_acc=0.873 | train_loss=3.4703\n",
      "Epoch  3/10: val_acc=0.897 | train_loss=1.5459\n",
      "Epoch  4/10: val_acc=0.921 | train_loss=0.7575\n",
      "Epoch  5/10: val_acc=0.921 | train_loss=0.6208\n",
      "Epoch  6/10: val_acc=0.929 | train_loss=0.3313\n",
      "Epoch  7/10: val_acc=0.897 | train_loss=1.1810\n",
      "Epoch  8/10: val_acc=0.921 | train_loss=0.9870\n",
      "Epoch  9/10: val_acc=0.929 | train_loss=0.7696\n",
      "Epoch 10/10: val_acc=0.921 | train_loss=0.3669\n",
      "\n",
      "‚úÖ CNN training completed!\n"
     ]
    }
   ],
   "source": [
    "# ============ CNN CLASSIFIER (ResNet18) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CNN (RESNET18) - BINARY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PedCropDataset(Dataset):\n",
    "    def __init__(self, pos_folder, neg_folder):\n",
    "        self.pos_paths = sorted(glob.glob(os.path.join(pos_folder, \"*.png\")))\n",
    "        self.neg_paths = sorted(glob.glob(os.path.join(neg_folder, \"*.png\")))\n",
    "        self.paths = self.pos_paths + self.neg_paths\n",
    "        self.labels = [1] * len(self.pos_paths) + [0] * len(self.neg_paths)\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "        y = self.labels[i]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "ds_cnn = PedCropDataset(pos_dir, neg_dir)\n",
    "n_cnn = len(ds_cnn)\n",
    "n_train_cnn = int(0.8 * n_cnn)\n",
    "train_ds_cnn, val_ds_cnn = torch.utils.data.random_split(ds_cnn, [n_train_cnn, n_cnn - n_train_cnn])\n",
    "train_dl_cnn = DataLoader(train_ds_cnn, batch_size=32, shuffle=True)\n",
    "val_dl_cnn   = DataLoader(val_ds_cnn, batch_size=32)\n",
    "\n",
    "print(f\"üìä Dataset: {len(train_ds_cnn)} train + {len(val_ds_cnn)} val\")\n",
    "print(f\"   Positive: {len(PedCropDataset(pos_dir, neg_dir).pos_paths)}\")\n",
    "print(f\"   Negative: {len(PedCropDataset(pos_dir, neg_dir).neg_paths)}\")\n",
    "\n",
    "# Build model\n",
    "model = models.resnet18(weights=None, num_classes=2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_dl_cnn:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot, correct = 0, 0\n",
    "        for xb, yb in val_dl_cnn:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            tot += yb.numel()\n",
    "            correct += (pred == yb).sum().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/10: val_acc={correct/tot:.3f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc9b760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:47:09.092923Z",
     "iopub.status.busy": "2025-12-21T03:47:09.092681Z",
     "iopub.status.idle": "2025-12-21T03:49:22.700610Z",
     "shell.execute_reply": "2025-12-21T03:49:22.699876Z",
     "shell.execute_reply.started": "2025-12-21T03:47:09.092902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì¶ FASTER R-CNN - OBJECT DETECTION\n",
      "================================================================================\n",
      "üìä Detection dataset: 136 train + 34 val\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160M/160M [00:02<00:00, 64.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:22<00:00,  3.06it/s, loss=0.0782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/6 completed in 22.2s | Avg Loss: 16.4711\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:21<00:00,  3.18it/s, loss=0.0430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2/6 completed in 21.4s | Avg Loss: 8.2749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:21<00:00,  3.17it/s, loss=0.0949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3/6 completed in 21.5s | Avg Loss: 5.8571\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:21<00:00,  3.16it/s, loss=0.0423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4/6 completed in 21.5s | Avg Loss: 4.9238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:21<00:00,  3.14it/s, loss=0.0492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5/6 completed in 21.7s | Avg Loss: 4.1458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:21<00:00,  3.13it/s, loss=0.0482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6/6 completed in 21.7s | Avg Loss: 4.1964\n",
      "\n",
      "‚úÖ Faster R-CNN training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============ FASTER R-CNN DETECTOR ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ FASTER R-CNN - OBJECT DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanDet(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir):\n",
    "        self.imgs = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.tf = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "def collate(batch): \n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "full_det = PennFudanDet(img_dir, mask_dir)\n",
    "n_det = len(full_det)\n",
    "n_train_det = int(0.8 * n_det)\n",
    "train_ds_det, val_ds_det = torch.utils.data.random_split(full_det, [n_train_det, n_det - n_train_det])\n",
    "train_dl_det = DataLoader(train_ds_det, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_det = DataLoader(val_ds_det, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Detection dataset: {n_train_det} train + {n_det - n_train_det} val\")\n",
    "\n",
    "# Build model\n",
    "det_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = det_model.roi_heads.box_predictor.cls_score.in_features\n",
    "det_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "det_model = det_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in det_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    det_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_det, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = det_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Faster R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ecd897c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:49:22.701655Z",
     "iopub.status.busy": "2025-12-21T03:49:22.701451Z",
     "iopub.status.idle": "2025-12-21T03:51:45.815946Z",
     "shell.execute_reply": "2025-12-21T03:51:45.815208Z",
     "shell.execute_reply.started": "2025-12-21T03:49:22.701633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üé≠ MASK R-CNN - INSTANCE SEGMENTATION\n",
      "================================================================================\n",
      "üìä Segmentation dataset: 136 train + 34 val\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:00<00:00, 185MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.91it/s, loss=0.2998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/6 completed in 23.4s | Avg Loss: 33.0030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.89it/s, loss=0.1349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2/6 completed in 23.6s | Avg Loss: 16.5076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.89it/s, loss=0.2344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3/6 completed in 23.5s | Avg Loss: 13.4479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.88it/s, loss=0.1663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4/6 completed in 23.6s | Avg Loss: 11.6992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.89it/s, loss=0.2353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5/6 completed in 23.6s | Avg Loss: 10.8867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:23<00:00,  2.89it/s, loss=0.1614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6/6 completed in 23.5s | Avg Loss: 10.3683\n",
      "\n",
      "‚úÖ Mask R-CNN training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============ MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ MASK R-CNN - INSTANCE SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanSeg(PennFudanDet):\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "full_seg = PennFudanSeg(img_dir, mask_dir)\n",
    "n_seg = len(full_seg)\n",
    "n_train_seg = int(0.8 * n_seg)\n",
    "train_ds_seg, val_ds_seg = torch.utils.data.random_split(full_seg, [n_train_seg, n_seg - n_train_seg])\n",
    "train_dl_seg = DataLoader(train_ds_seg, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_seg = DataLoader(val_ds_seg, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Segmentation dataset: {n_train_seg} train + {n_seg - n_train_seg} val\")\n",
    "\n",
    "# Build model\n",
    "seg_model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features_mask = seg_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden = 256\n",
    "seg_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden, 2)\n",
    "in_features = seg_model.roi_heads.box_predictor.cls_score.in_features\n",
    "seg_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "seg_model = seg_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in seg_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    seg_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_seg, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = seg_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Mask R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2a84ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:26:51.583716Z",
     "iopub.status.busy": "2025-12-21T06:26:51.583413Z",
     "iopub.status.idle": "2025-12-21T06:26:52.083421Z",
     "shell.execute_reply": "2025-12-21T06:26:52.082806Z",
     "shell.execute_reply.started": "2025-12-21T06:26:51.583680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üé® DEMO: CNN Classification Results\n",
      "================================================================================\n",
      "‚úÖ Saved: /kaggle/working/CNN_Results.png\n"
     ]
    }
   ],
   "source": [
    "# ============ VISUALIZATION: CNN RESULTS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé® DEMO: CNN Classification Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch, sample_labels = next(iter(val_dl_cnn))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    predictions = model(sample_batch)\n",
    "    predicted_classes = predictions.argmax(1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('CNN Classification Results (ResNet18)', fontsize=14, fontweight='bold')\n",
    "for idx in range(8):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    img = sample_batch[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    pred = predicted_classes[idx].item()\n",
    "    label = sample_labels[idx].item()\n",
    "    color = 'green' if pred == label else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {label}', color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'CNN_Results.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e375a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:26:52.085670Z",
     "iopub.status.busy": "2025-12-21T06:26:52.085432Z",
     "iopub.status.idle": "2025-12-21T06:26:53.271707Z",
     "shell.execute_reply": "2025-12-21T06:26:53.270936Z",
     "shell.execute_reply.started": "2025-12-21T06:26:52.085648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì¶ DEMO: Faster R-CNN Detection\n",
      "================================================================================\n",
      "‚úÖ Saved: /kaggle/working/RCNN_Detection.png\n"
     ]
    }
   ],
   "source": [
    "# ============ VISUALIZATION: FASTER R-CNN DETECTION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ DEMO: Faster R-CNN Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "det_model.eval()\n",
    "sample_imgs, sample_targets = next(iter(val_dl_det))\n",
    "sample_imgs_device = [im.to(device) for im in sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = det_model(sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Faster R-CNN Detection Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    ax = axes[idx]\n",
    "    img = sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Ground truth\n",
    "    for box in sample_targets[idx]['boxes'].cpu().numpy():\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Predictions\n",
    "    pred = predictions[idx]\n",
    "    for score, box in zip(pred['scores'].cpu().numpy(), pred['boxes'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_title(f'Image {idx+1}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'RCNN_Detection.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4625e50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:26:53.273142Z",
     "iopub.status.busy": "2025-12-21T06:26:53.272659Z",
     "iopub.status.idle": "2025-12-21T06:26:56.282489Z",
     "shell.execute_reply": "2025-12-21T06:26:56.281768Z",
     "shell.execute_reply.started": "2025-12-21T06:26:53.273120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üé≠ DEMO: Mask R-CNN Segmentation\n",
      "================================================================================\n",
      "‚úÖ Saved: /kaggle/working/MaskRCNN_Segmentation.png\n"
     ]
    }
   ],
   "source": [
    "# ============ VISUALIZATION: MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ DEMO: Mask R-CNN Segmentation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "seg_model.eval()\n",
    "seg_sample_imgs, seg_sample_targets = next(iter(val_dl_seg))\n",
    "seg_sample_imgs_device = [im.to(device) for im in seg_sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_predictions = seg_model(seg_sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Mask R-CNN Segmentation Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    # Ground truth\n",
    "    ax = axes[0, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Ground Truth - Image {idx+1}', fontweight='bold')\n",
    "    gt_masks = seg_sample_targets[idx]['masks'].cpu().numpy()\n",
    "    for mask in gt_masks:\n",
    "        ax.contour(mask, colors='green', linewidths=2)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    ax = axes[1, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Predictions - Image {idx+1}', fontweight='bold')\n",
    "    pred = seg_predictions[idx]\n",
    "    for mask, score in zip(pred['masks'].cpu().numpy(), pred['scores'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            ax.contour(mask.squeeze(), colors='red', linewidths=2, linestyles='--')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'MaskRCNN_Segmentation.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658b74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:26:57.904557Z",
     "iopub.status.busy": "2025-12-21T06:26:57.904230Z",
     "iopub.status.idle": "2025-12-21T06:26:58.722386Z",
     "shell.execute_reply": "2025-12-21T06:26:58.721737Z",
     "shell.execute_reply.started": "2025-12-21T06:26:57.904520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä MODEL PERFORMANCE ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 128241 (\\N{MOBILE PHONE}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 128230 (\\N{PACKAGE}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 127917 (\\N{PERFORMING ARTS}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 128260 (\\N{ANTICLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 128123 (\\N{GHOST}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:122: UserWarning: Glyph 128300 (\\N{MICROSCOPE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 128241 (\\N{MOBILE PHONE}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 128230 (\\N{PACKAGE}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 127917 (\\N{PERFORMING ARTS}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 128260 (\\N{ANTICLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 128123 (\\N{GHOST}) missing from font(s) DejaVu Sans Mono.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_55/3670256760.py:124: UserWarning: Glyph 128300 (\\N{MICROSCOPE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: /kaggle/working/Performance_Analysis.png\n",
      "\n",
      "üìà Model Statistics:\n",
      "   CNN            :    11.18M parameters\n",
      "   Faster R-CNN   :    41.08M parameters\n",
      "   Mask R-CNN     :    43.70M parameters\n",
      "   AE             :     7.38M parameters\n",
      "   GAN            :    12.65M parameters\n"
     ]
    }
   ],
   "source": [
    "# ============ PERFORMANCE ANALYSIS (CNN / Faster R-CNN / Mask R-CNN) ============\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE ANALYSIS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "\n",
    "# Calculate model parameters and sizes\n",
    "\n",
    "def count_parameters(model):\n",
    "\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "models_info = {\n",
    "\n",
    "    'CNN': (model, count_parameters(model)),\n",
    "\n",
    "    'Faster R-CNN': (det_model, count_parameters(det_model)),\n",
    "\n",
    "    'Mask R-CNN': (seg_model, count_parameters(seg_model)),\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create performance analysis figure\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Model Size Comparison\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "model_names = list(models_info.keys())\n",
    "\n",
    "param_counts = [models_info[name][1] / 1e6 for name in model_names]  # Millions\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "bars = ax1.bar(model_names, param_counts, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax1.set_ylabel('Parameters (Millions)', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax1.set_title('Model Size Comparison', fontweight='bold', fontsize=13)\n",
    "\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "for bar, val in zip(bars, param_counts):\n",
    "\n",
    "    height = bar.get_height()\n",
    "\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "\n",
    "             f'{val:.1f}M', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Task Capability Matrix\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "capabilities = {\n",
    "\n",
    "    'CNN': [1, 0, 0],\n",
    "\n",
    "    'Faster R-CNN': [1, 1, 0],\n",
    "\n",
    "    'Mask R-CNN': [1, 1, 1],\n",
    "\n",
    "}\n",
    "\n",
    "tasks = ['Classification', 'Detection', 'Segmentation']\n",
    "\n",
    "cap_matrix = np.array([capabilities[name] for name in capabilities.keys()])\n",
    "\n",
    "\n",
    "\n",
    "im = ax2.imshow(cap_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "ax2.set_xticks(range(len(tasks)))\n",
    "\n",
    "ax2.set_yticks(range(len(capabilities)))\n",
    "\n",
    "ax2.set_xticklabels(tasks, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "ax2.set_yticklabels(capabilities.keys(), fontsize=10)\n",
    "\n",
    "ax2.set_title('Task Capability Matrix', fontweight='bold', fontsize=13)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(capabilities)):\n",
    "\n",
    "    for j in range(len(tasks)):\n",
    "\n",
    "        _ = ax2.text(j, i, '‚úì' if cap_matrix[i, j] else '‚úó',\n",
    "\n",
    "                    ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Speed vs Quality Trade-off (illustrative)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "speed_fps = [15, 8, 7.5]  # Approximate FPS\n",
    "\n",
    "quality_acc = [85, 78, 80]  # Approximate quality/accuracy\n",
    "\n",
    "sizes = [param_counts[i]*50 for i in range(len(model_names))]\n",
    "\n",
    "\n",
    "\n",
    "scatter = ax3.scatter(speed_fps, quality_acc, s=sizes, c=range(len(model_names)),\n",
    "\n",
    "                     cmap='viridis', alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, name in enumerate(model_names):\n",
    "\n",
    "    ax3.annotate(name, (speed_fps[i], quality_acc[i]),\n",
    "\n",
    "                xytext=(5, 5), textcoords='offset points', fontweight='bold', fontsize=9)\n",
    "\n",
    "\n",
    "\n",
    "ax3.set_xlabel('Speed (FPS)', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax3.set_ylabel('Quality/Accuracy (%)', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax3.set_title('Speed vs Quality Trade-off', fontweight='bold', fontsize=13)\n",
    "\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax3.set_xlim(4, 18)\n",
    "\n",
    "ax3.set_ylim(70, 90)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Applications & Use Cases\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "ax4.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "use_cases_text = \"\"\"\n",
    "\n",
    "üì± APPLICATIONS & USE CASES\n",
    "\n",
    "\n",
    "\n",
    "üéØ CNN (ResNet18)\n",
    "\n",
    "   ‚Ä¢ Real-time pedestrian classification\n",
    "\n",
    "   ‚Ä¢ Cropped region validation\n",
    "\n",
    "   ‚Ä¢ Binary person/non-person detection\n",
    "\n",
    "\n",
    "\n",
    "üì¶ Faster R-CNN\n",
    "\n",
    "   ‚Ä¢ Crowd monitoring & surveillance\n",
    "\n",
    "   ‚Ä¢ Multi-person detection\n",
    "\n",
    "   ‚Ä¢ Speed-optimized deployment\n",
    "\n",
    "\n",
    "\n",
    "üé≠ Mask R-CNN\n",
    "\n",
    "   ‚Ä¢ Precise person segmentation\n",
    "\n",
    "   ‚Ä¢ Activity recognition\n",
    "\n",
    "   ‚Ä¢ Crowd counting with accuracy\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "ax4.text(0.05, 0.95, use_cases_text, transform=ax4.transAxes,\n",
    "\n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('üî¨ Model Performance Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(output_dir, 'Performance_Analysis.png')\n",
    "\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Print summary statistics\n",
    "\n",
    "print(\"\\nüìà Model Statistics:\")\n",
    "\n",
    "for name, (m, params) in models_info.items():\n",
    "\n",
    "    print(f\"   {name:15s}: {params/1e6:8.2f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9f608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:26:58.723523Z",
     "iopub.status.busy": "2025-12-21T06:26:58.723233Z",
     "iopub.status.idle": "2025-12-21T06:27:00.543714Z",
     "shell.execute_reply": "2025-12-21T06:27:00.543035Z",
     "shell.execute_reply.started": "2025-12-21T06:26:58.723498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üé¨ FULL COMPUTER VISION PIPELINE DEMO\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/2806630928.py:156: UserWarning: Glyph 127916 (\\N{CLAPPER BOARD}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_55/2806630928.py:158: UserWarning: Glyph 127916 (\\N{CLAPPER BOARD}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(output_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: /kaggle/working/FullPipeline_Demo.png\n",
      "\n",
      "üìä Full Pipeline Summary:\n",
      "   Original image: (1017, 444)\n",
      "   Detected persons: 4\n",
      "   Segmented masks: 4\n",
      "   Generated synthetic: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============ FULL PIPELINE DEMO (NO AE / NO GAN) ============\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"üé¨ FULL COMPUTER VISION PIPELINE DEMO + METRICS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "\n",
    "# L·∫•y 1 ·∫£nh test\n",
    "\n",
    "test_img_path = next(iter(glob.glob(os.path.join(img_dir, \"*.png\"))))\n",
    "\n",
    "test_img = Image.open(test_img_path).convert(\"RGB\")\n",
    "\n",
    "test_base = os.path.basename(test_img_path).replace(\".png\", \"\")\n",
    "\n",
    "test_mask_path = os.path.join(mask_dir, test_base + \"_mask.png\")\n",
    "\n",
    "test_mask = np.array(Image.open(test_mask_path))\n",
    "\n",
    "test_boxes, test_labels, test_masks = load_target(test_mask_path)\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 1: Original Image =====\n",
    "\n",
    "step1_img = np.array(test_img)\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 2: Ground Truth Mask =====\n",
    "\n",
    "step2_mask = test_mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 3: Ground Truth Bounding Boxes =====\n",
    "\n",
    "step3_img = np.array(test_img).copy()\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 4: Faster R-CNN Detection =====\n",
    "\n",
    "det_model.eval()\n",
    "\n",
    "test_img_tensor = transforms.ToTensor()(test_img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    det_preds = det_model(test_img_tensor)\n",
    "\n",
    "pred_boxes = det_preds[0]['boxes'].cpu()\n",
    "\n",
    "pred_scores = det_preds[0]['scores'].cpu()\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 5: Mask R-CNN Segmentation =====\n",
    "\n",
    "seg_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    seg_preds = seg_model(test_img_tensor)\n",
    "\n",
    "pred_masks = (seg_preds[0]['masks'].cpu() > 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 6: Combined Detection + Segmentation =====\n",
    "\n",
    "step6_img = np.array(test_img).copy()\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 7: CNN Input Crops + Predictions =====\n",
    "\n",
    "crop_samples = []\n",
    "\n",
    "crop_preds = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, b in enumerate(test_boxes[:2]):  # 2 ng∆∞·ªùi ƒë·∫ßu ti√™n\n",
    "\n",
    "    x1, y1, x2, y2 = map(int, b.tolist())\n",
    "\n",
    "    crop = test_img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    crop = transforms.Resize((64,64))(crop)\n",
    "\n",
    "    crop_tensor = transforms.ToTensor()(crop).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = model(crop_tensor).argmax(1).item()\n",
    "\n",
    "    crop_samples.append(np.array(crop))\n",
    "\n",
    "    crop_preds.append(pred)\n",
    "\n",
    "\n",
    "\n",
    "# ===== METRICS: Detection (IoU/Precision/Recall) =====\n",
    "\n",
    "iou_mean = 0.0\n",
    "\n",
    "precision = 0.0\n",
    "\n",
    "recall = 0.0\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "pred_keep = pred_boxes[pred_scores > thr]\n",
    "\n",
    "if len(pred_keep) > 0 and len(test_boxes) > 0:\n",
    "\n",
    "    ious = box_iou(pred_keep, test_boxes)\n",
    "\n",
    "    best_pred = ious.max(dim=1).values\n",
    "\n",
    "    iou_mean = best_pred.mean().item()\n",
    "\n",
    "    tp = (best_pred > 0.5).sum().item()\n",
    "\n",
    "    precision = tp / max(len(pred_keep), 1)\n",
    "\n",
    "    recall = tp / max(len(test_boxes), 1)\n",
    "\n",
    "\n",
    "\n",
    "# ===== METRICS: Segmentation IoU =====\n",
    "\n",
    "mask_iou = 0.0\n",
    "\n",
    "if pred_masks.shape[0] > 0 and test_masks.shape[0] > 0:\n",
    "\n",
    "    per_gt = []\n",
    "\n",
    "    for gt_mask in test_masks:\n",
    "\n",
    "        gt = gt_mask.bool().cpu().numpy()\n",
    "\n",
    "        best = 0.0\n",
    "\n",
    "        for pm in pred_masks:\n",
    "\n",
    "            pm_bin = pm.squeeze().numpy() > 0.5\n",
    "\n",
    "            inter = np.logical_and(pm_bin, gt).sum()\n",
    "\n",
    "            union = np.logical_or(pm_bin, gt).sum()\n",
    "\n",
    "            best = max(best, inter / (union + 1e-6))\n",
    "\n",
    "        per_gt.append(best)\n",
    "\n",
    "    mask_iou = float(np.mean(per_gt))\n",
    "\n",
    "\n",
    "\n",
    "# ===== Visualization =====\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n",
    "\n",
    "fig.suptitle('üé¨ Computer Vision Models - Full Pipeline Demo', fontsize=16, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# 1. Original Image\n",
    "\n",
    "ax = axes[0, 0]\n",
    "\n",
    "ax.imshow(step1_img)\n",
    "\n",
    "ax.set_title('1. Original Image', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Ground Truth Mask\n",
    "\n",
    "ax = axes[0, 1]\n",
    "\n",
    "ax.imshow(step2_mask, cmap='tab20')\n",
    "\n",
    "ax.set_title('2. Ground Truth Mask', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 3. GT Bounding Boxes\n",
    "\n",
    "ax = axes[0, 2]\n",
    "\n",
    "ax.imshow(step3_img)\n",
    "\n",
    "for box in test_boxes:\n",
    "\n",
    "    x1, y1, x2, y2 = map(int, box.tolist())\n",
    "\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.text(x1, y1-5, 'Person', color='green', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_title('3. GT Bounding Boxes', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 4. Faster R-CNN Detections\n",
    "\n",
    "ax = axes[1, 0]\n",
    "\n",
    "ax.imshow(step1_img)\n",
    "\n",
    "for i, (box, score) in enumerate(zip(pred_boxes, pred_scores)):\n",
    "\n",
    "    if score > thr:\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(x1, y1-5, f'{score:.2f}', color='red', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_title('4. Faster R-CNN Detections', fontweight='bold', fontsize=11, color='darkred')\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 5. Mask R-CNN Segmentation\n",
    "\n",
    "ax = axes[1, 1]\n",
    "\n",
    "ax.imshow(step1_img)\n",
    "\n",
    "for i, mask in enumerate(pred_masks):\n",
    "\n",
    "    if mask.max() > 0:\n",
    "\n",
    "        ax.contour(mask.squeeze(), colors=['cyan', 'magenta'][i % 2], linewidths=2)\n",
    "\n",
    "ax.set_title('5. Mask R-CNN Segmentation', fontweight='bold', fontsize=11, color='darkblue')\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 6. Combined Detection + Segmentation\n",
    "\n",
    "ax = axes[1, 2]\n",
    "\n",
    "ax.imshow(step1_img)\n",
    "\n",
    "for box in pred_keep:\n",
    "\n",
    "    x1, y1, x2, y2 = map(int, box.tolist())\n",
    "\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='yellow', facecolor='none')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "for i, mask in enumerate(pred_masks[:2]):\n",
    "\n",
    "    if mask.max() > 0:\n",
    "\n",
    "        ax.contour(mask.squeeze(), colors=['white', 'orange'][i % 2], linewidths=1, linestyles='--')\n",
    "\n",
    "ax.text(5, 20, f'IoU={iou_mean:.3f}\\nPrec={precision:.2f} | Rec={recall:.2f}\\nMask IoU={mask_iou:.3f}',\n",
    "\n",
    "        color='black', fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))\n",
    "\n",
    "ax.set_title('6. Combined + Metrics', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 7. CNN Input Crops\n",
    "\n",
    "ax = axes[2, 0]\n",
    "\n",
    "if crop_samples:\n",
    "\n",
    "    ax.imshow(crop_samples[0])\n",
    "\n",
    "    ax.set_title(f'7. CNN Crop (pred={crop_preds[0]})', fontweight='bold', fontsize=11, color='darkorange')\n",
    "\n",
    "else:\n",
    "\n",
    "    ax.text(0.5, 0.5, 'No crops', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_title('7. CNN Input Crops', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 8. Additional Crop (if available)\n",
    "\n",
    "ax = axes[2, 1]\n",
    "\n",
    "if len(crop_samples) > 1:\n",
    "\n",
    "    ax.imshow(crop_samples[1])\n",
    "\n",
    "    ax.set_title(f'8. CNN Crop (pred={crop_preds[1]})', fontweight='bold', fontsize=11, color='darkorange')\n",
    "\n",
    "else:\n",
    "\n",
    "    ax.text(0.5, 0.5, 'No 2nd crop', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_title('8. CNN Crop', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# 9. Metrics Summary\n",
    "\n",
    "ax = axes[2, 2]\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "\n",
    "üìä METRICS (single image)\n",
    "\n",
    "Detection IoU (mean): {iou_mean:.3f}\n",
    "\n",
    "Detection Precision@0.5: {precision:.2f}\n",
    "\n",
    "Detection Recall@0.5: {recall:.2f}\n",
    "\n",
    "Mask IoU (mean per GT): {mask_iou:.3f}\n",
    "\n",
    "Crops predicted: {len(crop_preds)}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0, 1, summary_text, fontsize=11, fontfamily='monospace', va='top',\n",
    "\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'))\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(output_dir, 'FullPipeline_Demo.png')\n",
    "\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüìä Full Pipeline Summary:\")\n",
    "\n",
    "print(f\"   Original image: {test_img.size}\")\n",
    "\n",
    "print(f\"   Detected persons (score>{thr}): {len(pred_keep)}\")\n",
    "\n",
    "print(f\"   GT persons: {len(test_boxes)}\")\n",
    "\n",
    "print(f\"   Detection IoU (mean): {iou_mean:.3f}\")\n",
    "\n",
    "print(f\"   Precision@0.5: {precision:.2f} | Recall@0.5: {recall:.2f}\")\n",
    "\n",
    "print(f\"   Mask IoU (mean per GT): {mask_iou:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b927de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ QUANTITATIVE EVALUATION (CNN / Faster R-CNN / Mask R-CNN) ============\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"üìè QUANTITATIVE EVALUATION vs GROUND TRUTH\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "\n",
    "# 1) CNN ACCURACY ON VAL\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tot, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for xb, yb in val_dl_cnn:\n",
    "\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        pred = model(xb).argmax(1)\n",
    "\n",
    "        tot += yb.numel()\n",
    "\n",
    "        correct += (pred == yb).sum().item()\n",
    "\n",
    "cnn_acc = correct / max(tot, 1)\n",
    "\n",
    "\n",
    "\n",
    "# 2) DETECTION METRICS (IoU / Precision / Recall) ON VAL\n",
    "\n",
    "det_model.eval()\n",
    "\n",
    "ious_all, prec_all, rec_all = [], [], []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for imgs, targets in val_dl_det:\n",
    "\n",
    "        imgs_dev = [im.to(device) for im in imgs]\n",
    "\n",
    "        preds = det_model(imgs_dev)\n",
    "\n",
    "        for pred, tgt in zip(preds, targets):\n",
    "\n",
    "            gt_boxes = tgt['boxes'].to(device)\n",
    "\n",
    "            keep = pred['scores'] > thr\n",
    "\n",
    "            pr_boxes = pred['boxes'][keep].to(device)\n",
    "\n",
    "            if len(gt_boxes) == 0:\n",
    "\n",
    "                continue\n",
    "\n",
    "            if len(pr_boxes) == 0:\n",
    "\n",
    "                prec_all.append(0.0)\n",
    "\n",
    "                rec_all.append(0.0)\n",
    "\n",
    "                ious_all.append(0.0)\n",
    "\n",
    "                continue\n",
    "\n",
    "            ious = box_iou(pr_boxes, gt_boxes)\n",
    "\n",
    "            best_pred = ious.max(dim=1).values\n",
    "\n",
    "            ious_all.extend(best_pred.detach().cpu().tolist())\n",
    "\n",
    "            tp = (best_pred > 0.5).sum().item()\n",
    "\n",
    "            prec_all.append(tp / max(len(pr_boxes), 1))\n",
    "\n",
    "            rec_all.append(tp / max(len(gt_boxes), 1))\n",
    "\n",
    "\n",
    "\n",
    "det_iou_mean = float(np.mean(ious_all)) if ious_all else 0.0\n",
    "\n",
    "det_prec = float(np.mean(prec_all)) if prec_all else 0.0\n",
    "\n",
    "det_rec = float(np.mean(rec_all)) if rec_all else 0.0\n",
    "\n",
    "\n",
    "\n",
    "# 3) SEGMENTATION METRIC (Mask IoU) ON VAL\n",
    "\n",
    "seg_model.eval()\n",
    "\n",
    "mask_ious = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for imgs, targets in val_dl_seg:\n",
    "\n",
    "        imgs_dev = [im.to(device) for im in imgs]\n",
    "\n",
    "        preds = seg_model(imgs_dev)\n",
    "\n",
    "        for pred, tgt in zip(preds, targets):\n",
    "\n",
    "            gt_masks = tgt['masks'].cpu().numpy()\n",
    "\n",
    "            pr_masks = (pred['masks'].cpu() > 0.5).float().numpy()\n",
    "\n",
    "            if gt_masks.shape[0] == 0:\n",
    "\n",
    "                continue\n",
    "\n",
    "            per_gt = []\n",
    "\n",
    "            for gm in gt_masks:\n",
    "\n",
    "                gm_bool = gm.astype(bool)\n",
    "\n",
    "                best = 0.0\n",
    "\n",
    "                for pm in pr_masks:\n",
    "\n",
    "                    pm_bool = pm.squeeze() > 0.5\n",
    "\n",
    "                    inter = np.logical_and(pm_bool, gm_bool).sum()\n",
    "\n",
    "                    union = np.logical_or(pm_bool, gm_bool).sum()\n",
    "\n",
    "                    best = max(best, inter / (union + 1e-6))\n",
    "\n",
    "                per_gt.append(best)\n",
    "\n",
    "            if per_gt:\n",
    "\n",
    "                mask_ious.append(np.mean(per_gt))\n",
    "\n",
    "mask_iou_mean = float(np.mean(mask_ious)) if mask_ious else 0.0\n",
    "\n",
    "\n",
    "\n",
    "# ---- VISUALIZE METRICS ----\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "metrics = ['CNN Acc', 'Det IoU', 'Det Prec', 'Det Rec', 'Mask IoU']\n",
    "\n",
    "values = [cnn_acc, det_iou_mean, det_prec, det_rec, mask_iou_mean]\n",
    "\n",
    "bars = ax.bar(metrics, values, color=['#4ECDC4', '#FF6B6B', '#FFB347', '#6A5ACD', '#45B7D1'], edgecolor='black')\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_ylabel('Score (0-1)', fontweight='bold')\n",
    "\n",
    "ax.set_title('Quantitative Metrics vs Ground Truth', fontweight='bold')\n",
    "\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., val + 0.02, f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "metrics_path = os.path.join(output_dir, 'Metrics_Summary.png')\n",
    "\n",
    "plt.savefig(metrics_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(f\"‚úÖ Saved metrics chart: {metrics_path}\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüìà METRIC SUMMARY (validation)\")\n",
    "\n",
    "print(f\"   CNN accuracy:           {cnn_acc:.3f}\")\n",
    "\n",
    "print(f\"   Detection IoU (mean):   {det_iou_mean:.3f}\")\n",
    "\n",
    "print(f\"   Detection Precision:    {det_prec:.3f}\")\n",
    "\n",
    "print(f\"   Detection Recall:       {det_rec:.3f}\")\n",
    "\n",
    "print(f\"   Mask IoU (mean):        {mask_iou_mean:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afedae4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:27:00.544905Z",
     "iopub.status.busy": "2025-12-21T06:27:00.544631Z",
     "iopub.status.idle": "2025-12-21T06:27:01.474567Z",
     "shell.execute_reply": "2025-12-21T06:27:01.473953Z",
     "shell.execute_reply.started": "2025-12-21T06:27:00.544883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß† CNN FEATURE MAP VISUALIZATION (Intermediate Layers)\n",
      "================================================================================\n",
      "‚úÖ Saved: /kaggle/working/CNN_FeatureMap_Visualization.png\n",
      "\n",
      "üìà Feature Map Analysis:\n",
      "   Total layers analyzed: 20\n",
      "   Visualized feature maps: 120\n",
      "   Purpose: Understanding what CNN learns at different depths\n"
     ]
    }
   ],
   "source": [
    "# ============ CNN FEATURE MAP VISUALIZATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† CNN FEATURE MAP VISUALIZATION (Intermediate Layers)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hook ƒë·ªÉ l·∫•y intermediate feature maps\n",
    "feature_maps = {}\n",
    "\n",
    "def get_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_maps[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks tr√™n c√°c layer c·ªßa ResNet18\n",
    "model.eval()\n",
    "\n",
    "# Hook v√†o t·∫•t c·∫£ conv layers\n",
    "hook_handles = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        h = module.register_forward_hook(get_hook(name))\n",
    "        hook_handles.append(h)\n",
    "\n",
    "# Select 8 random crops t·ª´ validation set\n",
    "sample_indices = np.random.choice(len(val_ds_cnn), 8, replace=False)\n",
    "sample_crops = [val_ds_cnn[i][0].unsqueeze(0) for i in sample_indices[:8]]\n",
    "\n",
    "# L·∫•y feature maps t·ª´ 8 crops\n",
    "all_feature_maps = []\n",
    "\n",
    "for crop_idx, crop in enumerate(sample_crops):\n",
    "    feature_maps.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(crop.to(device))\n",
    "    \n",
    "    # L·∫•y layer cu·ªëi (layer.2)\n",
    "    for name, feat in feature_maps.items():\n",
    "        if 'layer' in name and feat.shape[2] <= 8 and feat.shape[2] > 1:\n",
    "            # Normalize ƒë·ªÉ visualization\n",
    "            feat_norm = (feat - feat.min()) / (feat.max() - feat.min() + 1e-8)\n",
    "            all_feature_maps.append(feat_norm)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, 8, figsize=(18, 7))\n",
    "fig.suptitle('CNN Feature Map Visualization (Intermediate Layers)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        ax = axes[row, col]\n",
    "        idx = row * 8 + col\n",
    "        \n",
    "        if idx < len(all_feature_maps):\n",
    "            feat = all_feature_maps[idx]\n",
    "            # L·∫•y channel ƒë·∫ßu ti√™n ho·∫∑c average\n",
    "            if feat.shape[1] > 1:\n",
    "                feat_vis = feat[0, :3].mean(0).cpu().numpy()  # Average 3 channels\n",
    "            else:\n",
    "                feat_vis = feat[0, 0].cpu().numpy()\n",
    "            \n",
    "            # Visualize\n",
    "            im = ax.imshow(feat_vis, cmap='hot')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(f'Layer {idx//8+1} Ch{idx%8+1}', fontsize=8)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'CNN_FeatureMap_Visualization.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "# Remove hooks\n",
    "for h in hook_handles:\n",
    "    h.remove()\n",
    "\n",
    "print(\"\\nüìà Feature Map Analysis:\")\n",
    "print(f\"   Total layers analyzed: {len(hook_handles)}\")\n",
    "print(f\"   Visualized feature maps: {len(all_feature_maps)}\")\n",
    "print(f\"   Purpose: Understanding what CNN learns at different depths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ SAVE MODELS ============\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"üíæ SAVING MODELS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Save models to output directory\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'model_cnn.pth'))\n",
    "\n",
    "torch.save(det_model.state_dict(), os.path.join(output_dir, 'model_faster_rcnn.pth'))\n",
    "\n",
    "torch.save(seg_model.state_dict(), os.path.join(output_dir, 'model_mask_rcnn.pth'))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Models saved to: {output_dir}\")\n",
    "\n",
    "print(\"   - model_cnn.pth\")\n",
    "\n",
    "print(\"   - model_faster_rcnn.pth\")\n",
    "\n",
    "print(\"   - model_mask_rcnn.pth\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {output_dir}\")\n",
    "\n",
    "print(\"\\nüìä Generated files:\")\n",
    "\n",
    "for f in glob.glob(os.path.join(output_dir, '*.png')):\n",
    "\n",
    "    print(f\"   - {os.path.basename(f)}\")\n",
    "\n",
    "for f in glob.glob(os.path.join(output_dir, '*.pth')):\n",
    "\n",
    "    print(f\"   - {os.path.basename(f)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9082218,
     "sourceId": 14235815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
