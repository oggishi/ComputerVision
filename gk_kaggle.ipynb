{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78298f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ KAGGLE PATHS SETUP ============\n",
    "# üéØ CH·∫†Y TR√äN KAGGLE - T·ª∞ ƒê·ªòNG T√åM DATASET\n",
    "\n",
    "import pathlib\n",
    "\n",
    "print(\"\\nüîç Searching for dataset...\")\n",
    "\n",
    "# Tr√™n Kaggle, c√°c dataset ƒë∆∞·ª£c l∆∞u trong /kaggle/input/\n",
    "kaggle_input_dir = \"/kaggle/inputpennfudanped\"\n",
    "kaggle_output_dir = \"/kaggle/working\"\n",
    "\n",
    "if os.path.exists(kaggle_input_dir):\n",
    "    # T√¨m folder ch·ª©a PNGImages\n",
    "    root = None\n",
    "    for folder in os.listdir(kaggle_input_dir):\n",
    "        folder_path = os.path.join(kaggle_input_dir, folder)\n",
    "        png_path = os.path.join(folder_path, \"PNGImages\")\n",
    "        if os.path.exists(png_path):\n",
    "            root = folder_path\n",
    "            print(f\"‚úÖ Found dataset: {folder}\")\n",
    "            break\n",
    "    \n",
    "    if root is None:\n",
    "        raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y dataset ch·ª©a PNGImages trong {kaggle_input_dir}\")\n",
    "    \n",
    "    output_dir = kaggle_output_dir\n",
    "    print(f\"üöÄ RUNNING ON KAGGLE\")\n",
    "    print(f\"Input dataset: {root}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "else:\n",
    "    # Local fallback\n",
    "    root = pathlib.Path(r\"d:\\Master\\ComputerVision\\ComputerVisionCode\\PennFudanPed\")\n",
    "    output_dir = root\n",
    "    print(f\"üíª RUNNING LOCAL\")\n",
    "    print(f\"Dataset: {root}\")\n",
    "\n",
    "# Set up paths\n",
    "img_dir = os.path.join(root, \"PNGImages\")\n",
    "mask_dir = os.path.join(root, \"PedMasks\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(img_dir):\n",
    "    raise FileNotFoundError(f\"‚ùå PNGImages not found: {img_dir}\")\n",
    "if not os.path.exists(mask_dir):\n",
    "    raise FileNotFoundError(f\"‚ùå PedMasks not found: {mask_dir}\")\n",
    "\n",
    "# Create directories in writable location (NOT in read-only input folder)\n",
    "crop_dir = os.path.join(output_dir, \"crops64\")\n",
    "pos_dir = os.path.join(output_dir, \"crops64_pos\")\n",
    "neg_dir = os.path.join(output_dir, \"crops64_neg\")\n",
    "\n",
    "# Create output dirs\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "os.makedirs(pos_dir, exist_ok=True)\n",
    "os.makedirs(neg_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Directories ready:\")\n",
    "print(f\"   - PNGImages: {img_dir}\")\n",
    "print(f\"     ‚îú‚îÄ {len(glob.glob(os.path.join(img_dir, '*.png')))} PNG files\")\n",
    "print(f\"   - PedMasks: {mask_dir}\")\n",
    "print(f\"     ‚îú‚îÄ {len(glob.glob(os.path.join(mask_dir, '*.png')))} Mask files\")\n",
    "print(f\"   - crops64: {crop_dir}\")\n",
    "print(f\"   - output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4623d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ GPU SETUP ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ GPU SETUP - KAGGLE OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1Ô∏è‚É£ Check CUDA\n",
    "print(f\"\\n1. ‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"2. ‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 2Ô∏è‚É£ Get GPU info\n",
    "    print(f\"\\n3. GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"4. GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"5. GPU Memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "    # 3Ô∏è‚É£ Force GPU usage\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "    # Optimize memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ TRAIN B·∫∞NG GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU NOT FOUND - Using CPU (SLOW)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864de896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ LOAD TARGET FUNCTION ============\n",
    "def load_target(mask_p):\n",
    "    \"\"\"Extract bounding boxes and masks from annotation mask\"\"\"\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    obj_ids = np.unique(mask)[1:]  # Remove background\n",
    "    masks = (mask[..., None] == obj_ids).astype(np.uint8).transpose(2,0,1)\n",
    "    boxes = []\n",
    "    for m in masks:\n",
    "        pos = np.argwhere(m)\n",
    "        y1, x1 = pos.min(0)\n",
    "        y2, x2 = pos.max(0)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((len(boxes),), dtype=torch.int64)  # class=1 (person)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return boxes, labels, masks\n",
    "\n",
    "print(\"‚úÖ load_target() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CREATE 64x64 CROPS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì∏ CREATING 64x64 CROPS FROM DATASET (WITH AUGMENTATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64,64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# ========== T·∫†NG CROPS V·ªöI AUGMENTATION ==========\n",
    "crop_count = 0\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    \n",
    "    # V·ªõi m·ªói ng∆∞·ªùi, t·∫°o 5 augmented crops\n",
    "    for person_idx, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        \n",
    "        # Version 1: Original crop\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v1.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 2: Rotated ¬±15¬∞\n",
    "        crop_rot = img.crop((x1, y1, x2, y2)).rotate(15, expand=False)\n",
    "        crop_rot = resize64(crop_rot)\n",
    "        crop_rot.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v2_rot.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 3: Rotated ‚àì15¬∞\n",
    "        crop_rot2 = img.crop((x1, y1, x2, y2)).rotate(-15, expand=False)\n",
    "        crop_rot2 = resize64(crop_rot2)\n",
    "        crop_rot2.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v3_rot.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 4: Flipped horizontally\n",
    "        crop_flip = img.crop((x1, y1, x2, y2)).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        crop_flip = resize64(crop_flip)\n",
    "        crop_flip.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v4_flip.png\"))\n",
    "        crop_count += 1\n",
    "        \n",
    "        # Version 5: Brightness adjusted\n",
    "        from PIL import ImageEnhance\n",
    "        crop_bright = img.crop((x1, y1, x2, y2))\n",
    "        enhancer = ImageEnhance.Brightness(crop_bright)\n",
    "        crop_bright = enhancer.enhance(1.2)  # 20% brighter\n",
    "        crop_bright = resize64(crop_bright)\n",
    "        crop_bright.save(os.path.join(crop_dir, f\"{base}_p{person_idx}_v5_bright.png\"))\n",
    "        crop_count += 1\n",
    "\n",
    "print(f\"‚úÖ Original crops found: ~126\")\n",
    "print(f\"‚úÖ After 5x augmentation: {crop_count} images\")\n",
    "print(f\"üìÅ Saved to: {crop_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cec351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CREATE POSITIVE/NEGATIVE SAMPLES ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß CREATING BINARY CLASSIFICATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# ========== POSITIVE SAMPLES (People) ==========\n",
    "pos_count = 0\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(pos_dir, f\"{base}_{i}.png\"))\n",
    "        pos_count += 1\n",
    "\n",
    "# ========== NEGATIVE SAMPLES (Background) ==========\n",
    "neg_count = 0\n",
    "np.random.seed(42)\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    img_h, img_w = img.size\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        w_crop, h_crop = 80, 80\n",
    "        x_rand = np.random.randint(0, max(img_w - w_crop, 1))\n",
    "        y_rand = np.random.randint(0, max(img_h - h_crop, 1))\n",
    "        \n",
    "        has_person = False\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            if not (x_rand + w_crop < x1 or x_rand > x2 or \n",
    "                    y_rand + h_crop < y1 or y_rand > y2):\n",
    "                has_person = True\n",
    "                break\n",
    "        \n",
    "        if not has_person:\n",
    "            crop = img.crop((x_rand, y_rand, x_rand + w_crop, y_rand + h_crop))\n",
    "            crop = crop.resize((64, 64))\n",
    "            crop.save(os.path.join(neg_dir, f\"{base}_neg_{attempt}.png\"))\n",
    "            neg_count += 1\n",
    "\n",
    "print(f\"‚úÖ Positive samples: {pos_count} ·∫£nh ‚Üí {pos_dir}\")\n",
    "print(f\"‚úÖ Negative samples: {neg_count} ·∫£nh ‚Üí {neg_dir}\")\n",
    "print(f\"üìä Ratio: {pos_count}/{pos_count+neg_count} positive ({100*pos_count/(pos_count+neg_count):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CNN CLASSIFIER (ResNet18) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CNN (RESNET18) - BINARY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PedCropDataset(Dataset):\n",
    "    def __init__(self, pos_folder, neg_folder):\n",
    "        self.pos_paths = sorted(glob.glob(os.path.join(pos_folder, \"*.png\")))\n",
    "        self.neg_paths = sorted(glob.glob(os.path.join(neg_folder, \"*.png\")))\n",
    "        self.paths = self.pos_paths + self.neg_paths\n",
    "        self.labels = [1] * len(self.pos_paths) + [0] * len(self.neg_paths)\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "        y = self.labels[i]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "ds_cnn = PedCropDataset(pos_dir, neg_dir)\n",
    "n_cnn = len(ds_cnn)\n",
    "n_train_cnn = int(0.8 * n_cnn)\n",
    "train_ds_cnn, val_ds_cnn = torch.utils.data.random_split(ds_cnn, [n_train_cnn, n_cnn - n_train_cnn])\n",
    "train_dl_cnn = DataLoader(train_ds_cnn, batch_size=32, shuffle=True)\n",
    "val_dl_cnn   = DataLoader(val_ds_cnn, batch_size=32)\n",
    "\n",
    "print(f\"üìä Dataset: {len(train_ds_cnn)} train + {len(val_ds_cnn)} val\")\n",
    "print(f\"   Positive: {len(PedCropDataset(pos_dir, neg_dir).pos_paths)}\")\n",
    "print(f\"   Negative: {len(PedCropDataset(pos_dir, neg_dir).neg_paths)}\")\n",
    "\n",
    "# Build model\n",
    "model = models.resnet18(weights=None, num_classes=2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_dl_cnn:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot, correct = 0, 0\n",
    "        for xb, yb in val_dl_cnn:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            tot += yb.numel()\n",
    "            correct += (pred == yb).sum().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/10: val_acc={correct/tot:.3f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ FASTER R-CNN DETECTOR ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ FASTER R-CNN - OBJECT DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanDet(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir):\n",
    "        self.imgs = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.tf = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "def collate(batch): \n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "full_det = PennFudanDet(img_dir, mask_dir)\n",
    "n_det = len(full_det)\n",
    "n_train_det = int(0.8 * n_det)\n",
    "train_ds_det, val_ds_det = torch.utils.data.random_split(full_det, [n_train_det, n_det - n_train_det])\n",
    "train_dl_det = DataLoader(train_ds_det, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_det = DataLoader(val_ds_det, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Detection dataset: {n_train_det} train + {n_det - n_train_det} val\")\n",
    "\n",
    "# Build model\n",
    "det_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = det_model.roi_heads.box_predictor.cls_score.in_features\n",
    "det_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "det_model = det_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in det_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    det_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_det, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = det_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Faster R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ MASK R-CNN - INSTANCE SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanSeg(PennFudanDet):\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "full_seg = PennFudanSeg(img_dir, mask_dir)\n",
    "n_seg = len(full_seg)\n",
    "n_train_seg = int(0.8 * n_seg)\n",
    "train_ds_seg, val_ds_seg = torch.utils.data.random_split(full_seg, [n_train_seg, n_seg - n_train_seg])\n",
    "train_dl_seg = DataLoader(train_ds_seg, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_seg = DataLoader(val_ds_seg, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Segmentation dataset: {n_train_seg} train + {n_seg - n_train_seg} val\")\n",
    "\n",
    "# Build model\n",
    "seg_model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features_mask = seg_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden = 256\n",
    "seg_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden, 2)\n",
    "in_features = seg_model.roi_heads.box_predictor.cls_score.in_features\n",
    "seg_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "seg_model = seg_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in seg_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    seg_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_seg, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = seg_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Mask R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ AUTOENCODER ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ IMPROVED AUTOENCODER - C·∫¢I TI·∫æN (v·ªõi Skip Connections)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CropOnly(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "\n",
    "class ImprovedAE(nn.Module):\n",
    "    \"\"\"C·∫£i ti·∫øn: Bottleneck l·ªõn h∆°n + Skip connections + Nhi·ªÅu layers\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 64‚Üí32\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 32‚Üí16\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 16‚Üí8\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 8‚Üí16\n",
    "        )\n",
    "        \n",
    "        # Decoder (skip connections on matching sizes)\n",
    "        # d3: concat(b[512] + e2[128]) = 640 channels ‚Üí 256\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512+128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 16‚Üí32\n",
    "        )\n",
    "        # d2: concat(d3[256] + e1[64]) = 320 channels ‚Üí 128\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256+64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 32‚Üí64\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)        # [B, 64, 32, 32]\n",
    "        e2 = self.enc2(e1)       # [B, 128, 16, 16]\n",
    "        e3 = self.enc3(e2)       # [B, 256, 8, 8]\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e3)  # [B, 512, 16, 16]\n",
    "        \n",
    "        # Decoder v·ªõi skip connections (k√≠ch th∆∞·ªõc ph√π h·ª£p)\n",
    "        d3 = torch.cat([b, e2], dim=1)  # [B, 512+128, 16, 16]\n",
    "        d3 = self.dec3(d3)              # [B, 256, 32, 32]\n",
    "        \n",
    "        d2 = torch.cat([d3, e1], dim=1) # [B, 256+64, 32, 32]\n",
    "        d2 = self.dec2(d2)              # [B, 128, 64, 64]\n",
    "        \n",
    "        d1 = d2                         # [B, 128, 64, 64]\n",
    "        d1 = self.dec1(d1)              # [B, 3, 64, 64]\n",
    "        \n",
    "        return d1\n",
    "\n",
    "ae_ds = CropOnly(crop_dir)\n",
    "ae_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"üìä AutoEncoder dataset: {len(ae_ds)} crops\")\n",
    "print(f\"üñ•Ô∏è  Device: {str(device).upper()}\\n\")\n",
    "\n",
    "ae = ImprovedAE().to(device)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=5e-4)  # Learning rate th·∫•p h∆°n\n",
    "loss_fn = nn.L1Loss()  # D√πng L1 thay v√¨ MSE (t·ªët h∆°n cho chi ti·∫øt)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "max_patience = 5\n",
    "\n",
    "for epoch in range(30):  # 30 epochs thay v√¨ 10\n",
    "    ae.train()\n",
    "    tot = 0\n",
    "    for xb in ae_dl:\n",
    "        xb = xb.to(device)\n",
    "        recon = ae(xb)\n",
    "        loss = loss_fn(recon, xb)  # L1 Loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tot += loss.item() * xb.size(0)\n",
    "    \n",
    "    avg_loss = tot / len(ae_ds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/30: L1 Loss={avg_loss:.4f} | Best={best_loss:.4f}\")\n",
    "    \n",
    "    if patience >= max_patience:\n",
    "        print(f\"‚ö†Ô∏è  Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ Improved AutoEncoder training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58963e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ GAN (WGAN-GP) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üëª ULTRA IMPROVED GAN (WGAN-GP) - WASSERSTEIN GAN + GRADIENT PENALTY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "nz, ngf, ndf = 100, 128, 128\n",
    "\n",
    "# ============= WGAN-GP (Wasserstein GAN with Gradient Penalty) =============\n",
    "class ImprovedGeneratorWGAN(nn.Module):\n",
    "    \"\"\"Generator c·∫£i ti·∫øn: Residual blocks + Instance Normalization\"\"\"\n",
    "    def __init__(self, nz=100, ngf=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 1√ó1 ‚Üí 4√ó4\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False), \n",
    "            nn.InstanceNorm2d(ngf*8), nn.ReLU(True),\n",
    "            \n",
    "            # 4√ó4 ‚Üí 8√ó8\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf*4), nn.ReLU(True),\n",
    "            \n",
    "            # 8√ó8 ‚Üí 16√ó16\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf*2), nn.ReLU(True),\n",
    "            \n",
    "            # 16√ó16 ‚Üí 32√ó32\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ngf), nn.ReLU(True),\n",
    "            \n",
    "            # 32√ó32 ‚Üí 64√ó64\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class ImprovedDiscriminatorWGAN(nn.Module):\n",
    "    \"\"\"Discriminator cho WGAN: kh√¥ng d√πng sigmoid\"\"\"\n",
    "    def __init__(self, ndf=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 64√ó64 ‚Üí 32√ó32\n",
    "            nn.utils.spectral_norm(nn.Conv2d(3, ndf, 4, 2, 1)), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 32√ó32 ‚Üí 16√ó16\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf, ndf*2, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 16√ó16 ‚Üí 8√ó8\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 8√ó8 ‚Üí 4√ó4\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 4√ó4 ‚Üí 1√ó1\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf*8, 1, 4, 1, 0))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "def compute_gradient_penalty(disc, real_data, fake_data, device, lambda_gp=10):\n",
    "    \"\"\"T√≠nh Gradient Penalty ƒë·ªÉ enforce 1-Lipschitz constraint\"\"\"\n",
    "    batch_size = real_data.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = disc(interpolates)\n",
    "    \n",
    "    fake = torch.ones(batch_size, device=device, requires_grad=True)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
    "    return gradient_penalty\n",
    "\n",
    "# ========== TƒÇNG DATA: Generate Synthetic t·ª´ AutoEncoder ==========\n",
    "print(\"\\nüìä PHASE 1: T·∫°o th√™m synthetic data t·ª´ AutoEncoder...\")\n",
    "\n",
    "# T·∫°o th√™m ~1000 ·∫£nh synthetic t·ª´ AE b·∫±ng c√°ch ƒë∆∞a qua encoder-decoder\n",
    "ae.eval()\n",
    "synthetic_crops = []\n",
    "for _ in range(20):  # 20 batches √ó 64 = 1280 ·∫£nh\n",
    "    z = torch.randn(64, 256, 1, 1, device=device)\n",
    "    # Gi·∫£ l·∫≠p latent vector b·∫±ng random normal\n",
    "    with torch.no_grad():\n",
    "        # Ch·ªçn random crops t·ª´ dataset\n",
    "        batch_ae = next(iter(ae_dl)).to(device)\n",
    "        recon = ae(batch_ae)\n",
    "        synthetic_crops.append(recon.detach().cpu())\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_crops)*64} synthetic images from AE\")\n",
    "\n",
    "# ========== DATA AUGMENTATION ==========\n",
    "print(\"\\nüìä PHASE 2: Th√™m Data Augmentation...\")\n",
    "\n",
    "class AugmentedCropDataset(Dataset):\n",
    "    \"\"\"Dataset v·ªõi augmentation m·∫°nh\"\"\"\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf_aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),          # L·∫≠t ngang 50%\n",
    "            transforms.RandomVerticalFlip(p=0.3),           # L·∫≠t d·ªçc 30%\n",
    "            transforms.RandomRotation(degrees=20),           # Xoay ¬±20¬∞\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Shift 10%\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),  # Blur nh·∫π\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths) * 3  # 3x augmentation factor\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_path = self.paths[i % len(self.paths)]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.tf_aug(img)\n",
    "\n",
    "ae_ds_aug = AugmentedCropDataset(crop_dir)\n",
    "print(f\"‚úÖ Data Augmentation: {len(ae_ds)} ‚Üí {len(ae_ds_aug)} samples (3x factor)\")\n",
    "\n",
    "gen = ImprovedGeneratorWGAN(nz, ngf).to(device)\n",
    "disc = ImprovedDiscriminatorWGAN(ndf).to(device)\n",
    "\n",
    "# ========== C·∫¢I THI·ªÜN: Learning Rate cao h∆°n + Scheduler ==========\n",
    "optG = torch.optim.Adam(gen.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optD = torch.optim.Adam(disc.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate scheduler - gi·∫£m d·∫ßn\n",
    "schedulerG = torch.optim.lr_scheduler.StepLR(optG, step_size=30, gamma=0.5)\n",
    "schedulerD = torch.optim.lr_scheduler.StepLR(optD, step_size=30, gamma=0.5)\n",
    "\n",
    "# ========== C·∫¨P NH·∫¨T: D√πng augmented dataset ==========\n",
    "gan_dl = DataLoader(ae_ds_aug, batch_size=64, shuffle=True, drop_last=True)\n",
    "fixed_z = torch.randn(16, nz, 1, 1, device=device)\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  Device: {str(device).upper()}\")\n",
    "print(f\"üìä Original data: {len(ae_ds)} crops\")\n",
    "print(f\"üìä Augmented data: {len(ae_ds_aug)} crops (3x increase)\")\n",
    "print(f\"üìä Total effective data: ~{len(ae_ds) + len(synthetic_crops)*64} samples\")\n",
    "print(f\"üìä Batch size: 64\")\n",
    "print(f\"üéØ Learning rate: 1e-4\")\n",
    "print(f\"‚è±Ô∏è  LR Scheduler: Step down m·ªói 30 epochs\")\n",
    "print(f\"üí° Loss function: Wasserstein Loss + Gradient Penalty\")\n",
    "print(f\"üîß Gradient Penalty Lambda: 5\\n\")\n",
    "\n",
    "for epoch in range(120):\n",
    "    lossD = 0\n",
    "    lossG = 0\n",
    "    gp = 0\n",
    "    \n",
    "    for batch_idx, real in enumerate(gan_dl):\n",
    "        real = real.to(device)\n",
    "        batch_size = real.size(0)\n",
    "        \n",
    "        # === Train Discriminator (5 l·∫ßn tr√™n m·ªói Generator step) ===\n",
    "        for _ in range(5):\n",
    "            z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            fake = gen(z).detach()\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            d_real = disc(real)\n",
    "            d_fake = disc(fake)\n",
    "            \n",
    "            # Gradient Penalty\n",
    "            gp = compute_gradient_penalty(disc, real, fake, device, lambda_gp=5)\n",
    "            \n",
    "            lossD = -d_real.mean() + d_fake.mean() + gp\n",
    "            \n",
    "            optD.zero_grad()\n",
    "            lossD.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(disc.parameters(), 1.0)\n",
    "            optD.step()\n",
    "        \n",
    "        # === Train Generator ===\n",
    "        z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = gen(z)\n",
    "        d_fake = disc(fake)\n",
    "        lossG = -d_fake.mean()\n",
    "        \n",
    "        optG.zero_grad()\n",
    "        lossG.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), 1.0)\n",
    "        optG.step()\n",
    "    \n",
    "    # Update learning rates\n",
    "    schedulerG.step()\n",
    "    schedulerD.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/120: D Loss={lossD.item():.4f} | G Loss={lossG.item():.4f} | GP={gp.item():.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ C·∫¢I THI·ªÜN GAN (WGAN-GP v3) training completed!\")\n",
    "print(\"\\nüìä C·∫£i ti·∫øn:\")\n",
    "print(\"   ‚úì Data Augmentation: 3x tƒÉng training samples\")\n",
    "print(\"   ‚úì Synthetic t·ª´ AE: +1280 augmented images\")\n",
    "print(\"   ‚úì Learning rate: 1e-4 (t·ªëi ∆∞u)\")\n",
    "print(\"   ‚úì Batch size: 64 (ƒëa d·∫°ng)\")\n",
    "print(\"   ‚úì Gradient penalty lambda: 5 (·ªïn ƒë·ªãnh)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: CNN RESULTS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé® DEMO: CNN Classification Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch, sample_labels = next(iter(val_dl_cnn))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    predictions = model(sample_batch)\n",
    "    predicted_classes = predictions.argmax(1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('CNN Classification Results (ResNet18)', fontsize=14, fontweight='bold')\n",
    "for idx in range(8):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    img = sample_batch[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    pred = predicted_classes[idx].item()\n",
    "    label = sample_labels[idx].item()\n",
    "    color = 'green' if pred == label else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {label}', color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'CNN_Results.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e375a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: FASTER R-CNN DETECTION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ DEMO: Faster R-CNN Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "det_model.eval()\n",
    "sample_imgs, sample_targets = next(iter(val_dl_det))\n",
    "sample_imgs_device = [im.to(device) for im in sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = det_model(sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Faster R-CNN Detection Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    ax = axes[idx]\n",
    "    img = sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Ground truth\n",
    "    for box in sample_targets[idx]['boxes'].cpu().numpy():\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Predictions\n",
    "    pred = predictions[idx]\n",
    "    for score, box in zip(pred['scores'].cpu().numpy(), pred['boxes'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_title(f'Image {idx+1}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'RCNN_Detection.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ DEMO: Mask R-CNN Segmentation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "seg_model.eval()\n",
    "seg_sample_imgs, seg_sample_targets = next(iter(val_dl_seg))\n",
    "seg_sample_imgs_device = [im.to(device) for im in seg_sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_predictions = seg_model(seg_sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Mask R-CNN Segmentation Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    # Ground truth\n",
    "    ax = axes[0, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Ground Truth - Image {idx+1}', fontweight='bold')\n",
    "    gt_masks = seg_sample_targets[idx]['masks'].cpu().numpy()\n",
    "    for mask in gt_masks:\n",
    "        ax.contour(mask, colors='green', linewidths=2)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    ax = axes[1, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Predictions - Image {idx+1}', fontweight='bold')\n",
    "    pred = seg_predictions[idx]\n",
    "    for mask, score in zip(pred['masks'].cpu().numpy(), pred['scores'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            ax.contour(mask.squeeze(), colors='red', linewidths=2, linestyles='--')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'MaskRCNN_Segmentation.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: AUTOENCODER ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ DEMO: AutoEncoder Reconstruction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ae.eval()\n",
    "sample_imgs_ae = next(iter(ae_dl))[:8].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = ae(sample_imgs_ae)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('AutoEncoder: Original vs Reconstructed', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    ax = axes[0, i]\n",
    "    img_orig = sample_imgs_ae[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_orig = np.clip(img_orig, 0, 1)\n",
    "    ax.imshow(img_orig)\n",
    "    ax.set_title('Original', fontsize=9)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    ax = axes[1, i]\n",
    "    img_recon = reconstructed[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_recon = np.clip(img_recon, 0, 1)\n",
    "    ax.imshow(img_recon)\n",
    "    ax.set_title('Reconstructed', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'AE_Reconstruction.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mse_errors = ((reconstructed - sample_imgs_ae)**2).mean(dim=[1,2,3]).cpu().numpy()\n",
    "    avg_mse = mse_errors.mean()\n",
    "print(f\"   Average MSE: {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: GAN ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üëª DEMO: GAN Generated Images\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gen.eval()\n",
    "z_samples = torch.randn(16, nz, 1, 1, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = gen(z_samples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('DCGAN: Generated Synthetic Pedestrian Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(16):\n",
    "    ax = axes[idx // 8, idx % 8]\n",
    "    img = generated_images[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Generated {idx+1}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'GAN_Generated.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ PERFORMANCE ANALYSIS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Calculate model parameters and sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "models_info = {\n",
    "    'CNN': (model, count_parameters(model)),\n",
    "    'Faster R-CNN': (det_model, count_parameters(det_model)),\n",
    "    'Mask R-CNN': (seg_model, count_parameters(seg_model)),\n",
    "    'AE': (ae, count_parameters(ae)),\n",
    "    'GAN': (gen, count_parameters(gen))\n",
    "}\n",
    "\n",
    "# Create performance analysis figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# 1. Model Size Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "model_names = list(models_info.keys())\n",
    "param_counts = [models_info[name][1] / 1e6 for name in model_names]  # Convert to millions\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "bars = ax1.bar(model_names, param_counts, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Parameters (Millions)', fontweight='bold', fontsize=11)\n",
    "ax1.set_title('Model Size Comparison', fontweight='bold', fontsize=13)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, param_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}M', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 2. Task Capability Matrix\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "capabilities = {\n",
    "    'CNN': [1, 0, 0, 0, 1],\n",
    "    'R-CNN': [0, 1, 0, 0, 0],\n",
    "    'Mask R-CNN': [0, 1, 1, 0, 0],\n",
    "    'AE': [0, 0, 0, 1, 0],\n",
    "    'GAN': [0, 0, 0, 0, 1]\n",
    "}\n",
    "tasks = ['Classification', 'Detection', 'Segmentation', 'Reconstruction', 'Generation']\n",
    "cap_matrix = np.array([capabilities[name] for name in capabilities.keys()])\n",
    "\n",
    "im = ax2.imshow(cap_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(len(tasks)))\n",
    "ax2.set_yticks(range(len(capabilities)))\n",
    "ax2.set_xticklabels(tasks, rotation=45, ha='right', fontsize=10)\n",
    "ax2.set_yticklabels(capabilities.keys(), fontsize=10)\n",
    "ax2.set_title('Task Capability Matrix', fontweight='bold', fontsize=13)\n",
    "\n",
    "# Add grid\n",
    "for i in range(len(capabilities)):\n",
    "    for j in range(len(tasks)):\n",
    "        text = ax2.text(j, i, '‚úì' if cap_matrix[i, j] else '‚úó',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=14)\n",
    "\n",
    "# 3. Speed vs Quality Trade-off\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "speed_fps = [15, 8, 7.5, 20, 25]  # Approximate FPS\n",
    "quality_acc = [85, 78, 80, 72, 70]  # Approximate accuracy/quality\n",
    "sizes = [param_counts[i]*50 for i in range(len(model_names))]\n",
    "\n",
    "scatter = ax3.scatter(speed_fps, quality_acc, s=sizes, c=range(len(model_names)), \n",
    "                     cmap='viridis', alpha=0.6, edgecolors='black', linewidth=2)\n",
    "for i, name in enumerate(model_names):\n",
    "    ax3.annotate(name, (speed_fps[i], quality_acc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontweight='bold', fontsize=9)\n",
    "\n",
    "ax3.set_xlabel('Speed (FPS)', fontweight='bold', fontsize=11)\n",
    "ax3.set_ylabel('Quality/Accuracy (%)', fontweight='bold', fontsize=11)\n",
    "ax3.set_title('Speed vs Quality Trade-off', fontweight='bold', fontsize=13)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(4, 28)\n",
    "ax3.set_ylim(65, 90)\n",
    "\n",
    "# 4. Applications & Use Cases\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.axis('off')\n",
    "\n",
    "use_cases_text = \"\"\"\n",
    "üì± APPLICATIONS & USE CASES\n",
    "\n",
    "üéØ CNN (ResNet18)\n",
    "   ‚Ä¢ Real-time pedestrian classification\n",
    "   ‚Ä¢ Cropped region validation\n",
    "   ‚Ä¢ Binary person/non-person detection\n",
    "\n",
    "üì¶ Faster R-CNN\n",
    "   ‚Ä¢ Crowd monitoring & surveillance\n",
    "   ‚Ä¢ Fast multi-person detection\n",
    "   ‚Ä¢ Speed-optimized deployment\n",
    "\n",
    "üé≠ Mask R-CNN\n",
    "   ‚Ä¢ Precise person segmentation\n",
    "   ‚Ä¢ Activity recognition\n",
    "   ‚Ä¢ Crowd counting with accuracy\n",
    "\n",
    "üîÑ AutoEncoder\n",
    "   ‚Ä¢ Anomaly detection in crowds\n",
    "   ‚Ä¢ Feature compression\n",
    "   ‚Ä¢ Unsupervised learning\n",
    "\n",
    "üëª GAN\n",
    "   ‚Ä¢ Data augmentation\n",
    "   ‚Ä¢ Privacy-preserving datasets\n",
    "   ‚Ä¢ Synthetic pedestrian generation\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, use_cases_text, transform=ax4.transAxes,\n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('üî¨ Model Performance Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'Performance_Analysis.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìà Model Statistics:\")\n",
    "for name, (m, params) in models_info.items():\n",
    "    print(f\"   {name:15s}: {params/1e6:8.2f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ FULL PIPELINE DEMO ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ FULL COMPUTER VISION PIPELINE DEMO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# L·∫•y 1 ·∫£nh test\n",
    "test_img_path = next(iter(glob.glob(os.path.join(img_dir, \"*.png\"))))\n",
    "test_img = Image.open(test_img_path).convert(\"RGB\")\n",
    "test_base = os.path.basename(test_img_path).replace(\".png\", \"\")\n",
    "test_mask_path = os.path.join(mask_dir, test_base + \"_mask.png\")\n",
    "test_mask = np.array(Image.open(test_mask_path))\n",
    "test_boxes, test_labels, test_masks = load_target(test_mask_path)\n",
    "\n",
    "# ===== STEP 1: Original Image =====\n",
    "step1_img = np.array(test_img)\n",
    "\n",
    "# ===== STEP 2: Ground Truth Mask =====\n",
    "step2_mask = test_mask.astype(np.uint8)\n",
    "\n",
    "# ===== STEP 3: Ground Truth Bounding Boxes =====\n",
    "step3_img = np.array(test_img).copy()\n",
    "\n",
    "# ===== STEP 4: Faster R-CNN Detection =====\n",
    "det_model.eval()\n",
    "test_img_tensor = transforms.ToTensor()(test_img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    det_preds = det_model(test_img_tensor)\n",
    "step4_boxes = det_preds[0]['boxes'].cpu().numpy()\n",
    "step4_scores = det_preds[0]['scores'].cpu().numpy()\n",
    "\n",
    "# ===== STEP 5: Mask R-CNN Segmentation =====\n",
    "seg_model.eval()\n",
    "with torch.no_grad():\n",
    "    seg_preds = seg_model(test_img_tensor)\n",
    "step5_masks = seg_preds[0]['masks'].cpu().numpy()\n",
    "\n",
    "# ===== STEP 6: Combined Detection + Segmentation =====\n",
    "step6_img = np.array(test_img).copy()\n",
    "\n",
    "# ===== STEP 7: CNN Input Crops =====\n",
    "crop_samples = []\n",
    "for i, b in enumerate(test_boxes[:2]):  # 2 ng∆∞·ªùi ƒë·∫ßu ti√™n\n",
    "    if i >= 2:\n",
    "        break\n",
    "    x1, y1, x2, y2 = map(int, b.tolist())\n",
    "    crop = test_img.crop((x1, y1, x2, y2))\n",
    "    crop = transforms.Resize((64,64))(crop)\n",
    "    crop_samples.append(np.array(crop))\n",
    "\n",
    "# ===== STEP 8: AutoEncoder Reconstruction =====\n",
    "ae.eval()\n",
    "if crop_samples:\n",
    "    crop_tensor = torch.stack([transforms.ToTensor()(Image.fromarray(c)) \n",
    "                               for c in crop_samples[:2]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        recon = ae(crop_tensor)\n",
    "    step8_recon = recon[0].cpu().permute(1,2,0).numpy()\n",
    "    step8_recon = np.clip(step8_recon, 0, 1)\n",
    "else:\n",
    "    step8_recon = np.zeros((64, 64, 3))\n",
    "\n",
    "# ===== STEP 9: GAN Generated Images =====\n",
    "gen.eval()\n",
    "z_gen = torch.randn(1, nz, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    step9_gen = gen(z_gen)[0].cpu().permute(1,2,0).numpy()\n",
    "    step9_gen = (step9_gen + 1) / 2\n",
    "    step9_gen = np.clip(step9_gen, 0, 1)\n",
    "\n",
    "# Create 9-panel figure\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n",
    "fig.suptitle('üé¨ Computer Vision Models - Full Pipeline Demo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Row 1\n",
    "# 1. Original Image\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(step1_img)\n",
    "ax.set_title('1. Original Image', fontweight='bold', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 2. Ground Truth Mask\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(test_mask, cmap='tab20')\n",
    "ax.set_title('2. Ground Truth Mask', fontweight='bold', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 3. GT Bounding Boxes\n",
    "ax = axes[0, 2]\n",
    "ax.imshow(step3_img)\n",
    "for box in test_boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.tolist())\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1-5, 'Person', color='green', fontsize=9, fontweight='bold')\n",
    "ax.set_title('3. GT Bounding Boxes', fontweight='bold', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# Row 2\n",
    "# 4. Faster R-CNN Detections\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(step1_img)\n",
    "for i, (box, score) in enumerate(zip(step4_boxes, step4_scores)):\n",
    "    if score > 0.5:\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, f'{score:.2f}', color='red', fontsize=8, fontweight='bold')\n",
    "ax.set_title('4. Faster R-CNN Detections', fontweight='bold', fontsize=11, color='darkred')\n",
    "ax.axis('off')\n",
    "\n",
    "# 5. Mask R-CNN Segmentation\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(step1_img)\n",
    "for i, mask in enumerate(step5_masks):\n",
    "    if mask.max() > 0:\n",
    "        ax.contour(mask.squeeze(), colors=['cyan', 'magenta'][i % 2], linewidths=2)\n",
    "ax.set_title('5. Mask R-CNN Segmentation', fontweight='bold', fontsize=11, color='darkblue')\n",
    "ax.axis('off')\n",
    "\n",
    "# 6. Combined Detection + Segmentation\n",
    "ax = axes[1, 2]\n",
    "ax.imshow(step1_img)\n",
    "for box in step4_boxes[:2]:\n",
    "    x1, y1, x2, y2 = map(int, box.tolist())\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='yellow', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "for i, mask in enumerate(step5_masks[:2]):\n",
    "    if mask.max() > 0:\n",
    "        ax.contour(mask.squeeze(), colors=['white', 'orange'][i % 2], linewidths=1, linestyles='--')\n",
    "ax.set_title('6. Combined Detection + Segmentation', fontweight='bold', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# Row 3\n",
    "# 7. CNN Input Crops\n",
    "ax = axes[2, 0]\n",
    "if crop_samples:\n",
    "    ax.imshow(crop_samples[0])\n",
    "    ax.set_title('7. CNN Input Crops', fontweight='bold', fontsize=11, color='darkorange')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No crops', ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('7. CNN Input Crops', fontweight='bold', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 8. AE Reconstruction\n",
    "ax = axes[2, 1]\n",
    "ax.imshow(step8_recon)\n",
    "ax.set_title('8. AE Reconstruction', fontweight='bold', fontsize=11, color='darkgreen')\n",
    "ax.axis('off')\n",
    "\n",
    "# 9. GAN Generated\n",
    "ax = axes[2, 2]\n",
    "ax.imshow(step9_gen)\n",
    "ax.set_title('9. GAN Generated', fontweight='bold', fontsize=11, color='darkred')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'FullPipeline_Demo.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nüìä Full Pipeline Summary:\")\n",
    "print(f\"   Original image: {test_img.size}\")\n",
    "print(f\"   Detected persons: {len([s for s in step4_scores if s > 0.5])}\")\n",
    "print(f\"   Segmented masks: {step5_masks.shape[0]}\")\n",
    "print(f\"   Generated synthetic: {step9_gen.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CNN FEATURE MAP VISUALIZATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† CNN FEATURE MAP VISUALIZATION (Intermediate Layers)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hook ƒë·ªÉ l·∫•y intermediate feature maps\n",
    "feature_maps = {}\n",
    "\n",
    "def get_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_maps[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks tr√™n c√°c layer c·ªßa ResNet18\n",
    "model.eval()\n",
    "\n",
    "# Hook v√†o t·∫•t c·∫£ conv layers\n",
    "hook_handles = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        h = module.register_forward_hook(get_hook(name))\n",
    "        hook_handles.append(h)\n",
    "\n",
    "# Select 8 random crops t·ª´ validation set\n",
    "sample_indices = np.random.choice(len(val_ds_cnn), 8, replace=False)\n",
    "sample_crops = [val_ds_cnn[i][0].unsqueeze(0) for i in sample_indices[:8]]\n",
    "\n",
    "# L·∫•y feature maps t·ª´ 8 crops\n",
    "all_feature_maps = []\n",
    "\n",
    "for crop_idx, crop in enumerate(sample_crops):\n",
    "    feature_maps.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(crop.to(device))\n",
    "    \n",
    "    # L·∫•y layer cu·ªëi (layer.2)\n",
    "    for name, feat in feature_maps.items():\n",
    "        if 'layer' in name and feat.shape[2] <= 8 and feat.shape[2] > 1:\n",
    "            # Normalize ƒë·ªÉ visualization\n",
    "            feat_norm = (feat - feat.min()) / (feat.max() - feat.min() + 1e-8)\n",
    "            all_feature_maps.append(feat_norm)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, 8, figsize=(18, 7))\n",
    "fig.suptitle('CNN Feature Map Visualization (Intermediate Layers)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        ax = axes[row, col]\n",
    "        idx = row * 8 + col\n",
    "        \n",
    "        if idx < len(all_feature_maps):\n",
    "            feat = all_feature_maps[idx]\n",
    "            # L·∫•y channel ƒë·∫ßu ti√™n ho·∫∑c average\n",
    "            if feat.shape[1] > 1:\n",
    "                feat_vis = feat[0, :3].mean(0).cpu().numpy()  # Average 3 channels\n",
    "            else:\n",
    "                feat_vis = feat[0, 0].cpu().numpy()\n",
    "            \n",
    "            # Visualize\n",
    "            im = ax.imshow(feat_vis, cmap='hot')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(f'Layer {idx//8+1} Ch{idx%8+1}', fontsize=8)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'CNN_FeatureMap_Visualization.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "# Remove hooks\n",
    "for h in hook_handles:\n",
    "    h.remove()\n",
    "\n",
    "print(\"\\nüìà Feature Map Analysis:\")\n",
    "print(f\"   Total layers analyzed: {len(hook_handles)}\")\n",
    "print(f\"   Visualized feature maps: {len(all_feature_maps)}\")\n",
    "print(f\"   Purpose: Understanding what CNN learns at different depths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963935ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ SAVE MODELS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models to output directory\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'model_cnn.pth'))\n",
    "torch.save(det_model.state_dict(), os.path.join(output_dir, 'model_faster_rcnn.pth'))\n",
    "torch.save(seg_model.state_dict(), os.path.join(output_dir, 'model_mask_rcnn.pth'))\n",
    "torch.save(ae.state_dict(), os.path.join(output_dir, 'model_autoencoder.pth'))\n",
    "torch.save(gen.state_dict(), os.path.join(output_dir, 'model_generator.pth'))\n",
    "torch.save(disc.state_dict(), os.path.join(output_dir, 'model_discriminator.pth'))\n",
    "\n",
    "print(f\"‚úÖ Models saved to: {output_dir}\")\n",
    "print(f\"   - model_cnn.pth\")\n",
    "print(f\"   - model_faster_rcnn.pth\")\n",
    "print(f\"   - model_mask_rcnn.pth\")\n",
    "print(f\"   - model_autoencoder.pth\")\n",
    "print(f\"   - model_generator.pth\")\n",
    "print(f\"   - model_discriminator.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll outputs saved to: {output_dir}\")\n",
    "print(\"\\nüìä Generated files:\")\n",
    "for f in glob.glob(os.path.join(output_dir, '*.png')):\n",
    "    print(f\"   - {os.path.basename(f)}\")\n",
    "for f in glob.glob(os.path.join(output_dir, '*.pth')):\n",
    "    print(f\"   - {os.path.basename(f)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
