{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78298f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ KAGGLE PATHS SETUP ============\n",
    "# üéØ C√ì TH·ªÇ CH·∫†Y TR√äN KAGGLE HO·∫∂C LOCAL\n",
    "\n",
    "# Ki·ªÉm tra c√≥ ph·∫£i ch·∫°y tr√™n Kaggle kh√¥ng\n",
    "KAGGLE_INPUT = \"/kaggle/input/penfudanped\"\n",
    "KAGGLE_OUTPUT = \"/kaggle/working\"\n",
    "\n",
    "if os.path.exists(KAGGLE_INPUT):\n",
    "    root = KAGGLE_INPUT\n",
    "    output_dir = KAGGLE_OUTPUT\n",
    "    print(f\"\\nüöÄ RUNNING ON KAGGLE\")\n",
    "    print(f\"Input dataset: {root}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "else:\n",
    "    # Local fallback\n",
    "    root = r\"./PennFudanPed\"\n",
    "    output_dir = root\n",
    "    print(f\"\\nüíª RUNNING LOCAL\")\n",
    "    print(f\"Dataset: {root}\")\n",
    "\n",
    "# Set up paths\n",
    "img_dir = os.path.join(root, \"PNGImages\")\n",
    "mask_dir = os.path.join(root, \"PedMasks\")\n",
    "\n",
    "# Create directories in writable location (NOT in read-only input folder)\n",
    "crop_dir = os.path.join(output_dir, \"crops64\")\n",
    "pos_dir = os.path.join(output_dir, \"crops64_pos\")\n",
    "neg_dir = os.path.join(output_dir, \"crops64_neg\")\n",
    "\n",
    "# Create output dirs\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "os.makedirs(pos_dir, exist_ok=True)\n",
    "os.makedirs(neg_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Directories created:\")\n",
    "print(f\"   - PNGImages: {img_dir}\")\n",
    "print(f\"   - PedMasks: {mask_dir}\")\n",
    "print(f\"   - crops64: {crop_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4623d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ GPU SETUP ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ GPU SETUP - KAGGLE OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1Ô∏è‚É£ Check CUDA\n",
    "print(f\"\\n1. ‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"2. ‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 2Ô∏è‚É£ Get GPU info\n",
    "    print(f\"\\n3. GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"4. GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"5. GPU Memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "    # 3Ô∏è‚É£ Force GPU usage\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "    # Optimize memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ TRAIN B·∫∞NG GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU NOT FOUND - Using CPU (SLOW)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864de896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ LOAD TARGET FUNCTION ============\n",
    "def load_target(mask_p):\n",
    "    \"\"\"Extract bounding boxes and masks from annotation mask\"\"\"\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    obj_ids = np.unique(mask)[1:]  # Remove background\n",
    "    masks = (mask[..., None] == obj_ids).astype(np.uint8).transpose(2,0,1)\n",
    "    boxes = []\n",
    "    for m in masks:\n",
    "        pos = np.argwhere(m)\n",
    "        y1, x1 = pos.min(0)\n",
    "        y2, x2 = pos.max(0)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((len(boxes),), dtype=torch.int64)  # class=1 (person)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return boxes, labels, masks\n",
    "\n",
    "print(\"‚úÖ load_target() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CREATE 64x64 CROPS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì∏ CREATING 64x64 CROPS FROM DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64,64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# Duy·ªát qua t·ª´ng ·∫£nh\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(crop_dir, f\"{base}_{i}.png\"))\n",
    "\n",
    "print(f\"‚úÖ Created crops64: {len(glob.glob(os.path.join(crop_dir, '*.png')))} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cec351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CREATE POSITIVE/NEGATIVE SAMPLES ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß CREATING BINARY CLASSIFICATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# ========== POSITIVE SAMPLES (People) ==========\n",
    "pos_count = 0\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(pos_dir, f\"{base}_{i}.png\"))\n",
    "        pos_count += 1\n",
    "\n",
    "# ========== NEGATIVE SAMPLES (Background) ==========\n",
    "neg_count = 0\n",
    "np.random.seed(42)\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    img_h, img_w = img.size\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        w_crop, h_crop = 80, 80\n",
    "        x_rand = np.random.randint(0, max(img_w - w_crop, 1))\n",
    "        y_rand = np.random.randint(0, max(img_h - h_crop, 1))\n",
    "        \n",
    "        has_person = False\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            if not (x_rand + w_crop < x1 or x_rand > x2 or \n",
    "                    y_rand + h_crop < y1 or y_rand > y2):\n",
    "                has_person = True\n",
    "                break\n",
    "        \n",
    "        if not has_person:\n",
    "            crop = img.crop((x_rand, y_rand, x_rand + w_crop, y_rand + h_crop))\n",
    "            crop = crop.resize((64, 64))\n",
    "            crop.save(os.path.join(neg_dir, f\"{base}_neg_{attempt}.png\"))\n",
    "            neg_count += 1\n",
    "\n",
    "print(f\"‚úÖ Positive samples: {pos_count} ·∫£nh ‚Üí {pos_dir}\")\n",
    "print(f\"‚úÖ Negative samples: {neg_count} ·∫£nh ‚Üí {neg_dir}\")\n",
    "print(f\"üìä Ratio: {pos_count}/{pos_count+neg_count} positive ({100*pos_count/(pos_count+neg_count):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CNN CLASSIFIER (ResNet18) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CNN (RESNET18) - BINARY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PedCropDataset(Dataset):\n",
    "    def __init__(self, pos_folder, neg_folder):\n",
    "        self.pos_paths = sorted(glob.glob(os.path.join(pos_folder, \"*.png\")))\n",
    "        self.neg_paths = sorted(glob.glob(os.path.join(neg_folder, \"*.png\")))\n",
    "        self.paths = self.pos_paths + self.neg_paths\n",
    "        self.labels = [1] * len(self.pos_paths) + [0] * len(self.neg_paths)\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "        y = self.labels[i]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "ds_cnn = PedCropDataset(pos_dir, neg_dir)\n",
    "n_cnn = len(ds_cnn)\n",
    "n_train_cnn = int(0.8 * n_cnn)\n",
    "train_ds_cnn, val_ds_cnn = torch.utils.data.random_split(ds_cnn, [n_train_cnn, n_cnn - n_train_cnn])\n",
    "train_dl_cnn = DataLoader(train_ds_cnn, batch_size=32, shuffle=True)\n",
    "val_dl_cnn   = DataLoader(val_ds_cnn, batch_size=32)\n",
    "\n",
    "print(f\"üìä Dataset: {len(train_ds_cnn)} train + {len(val_ds_cnn)} val\")\n",
    "print(f\"   Positive: {len(PedCropDataset(pos_dir, neg_dir).pos_paths)}\")\n",
    "print(f\"   Negative: {len(PedCropDataset(pos_dir, neg_dir).neg_paths)}\")\n",
    "\n",
    "# Build model\n",
    "model = models.resnet18(weights=None, num_classes=2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_dl_cnn:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot, correct = 0, 0\n",
    "        for xb, yb in val_dl_cnn:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            tot += yb.numel()\n",
    "            correct += (pred == yb).sum().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/10: val_acc={correct/tot:.3f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ FASTER R-CNN DETECTOR ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ FASTER R-CNN - OBJECT DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanDet(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir):\n",
    "        self.imgs = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.tf = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "def collate(batch): \n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "full_det = PennFudanDet(img_dir, mask_dir)\n",
    "n_det = len(full_det)\n",
    "n_train_det = int(0.8 * n_det)\n",
    "train_ds_det, val_ds_det = torch.utils.data.random_split(full_det, [n_train_det, n_det - n_train_det])\n",
    "train_dl_det = DataLoader(train_ds_det, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_det = DataLoader(val_ds_det, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Detection dataset: {n_train_det} train + {n_det - n_train_det} val\")\n",
    "\n",
    "# Build model\n",
    "det_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = det_model.roi_heads.box_predictor.cls_score.in_features\n",
    "det_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "det_model = det_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in det_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    det_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_det, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = det_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Faster R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ MASK R-CNN - INSTANCE SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PennFudanSeg(PennFudanDet):\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base + \"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "full_seg = PennFudanSeg(img_dir, mask_dir)\n",
    "n_seg = len(full_seg)\n",
    "n_train_seg = int(0.8 * n_seg)\n",
    "train_ds_seg, val_ds_seg = torch.utils.data.random_split(full_seg, [n_train_seg, n_seg - n_train_seg])\n",
    "train_dl_seg = DataLoader(train_ds_seg, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl_seg = DataLoader(val_ds_seg, batch_size=2, collate_fn=collate)\n",
    "\n",
    "print(f\"üìä Segmentation dataset: {n_train_seg} train + {n_seg - n_train_seg} val\")\n",
    "\n",
    "# Build model\n",
    "seg_model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features_mask = seg_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden = 256\n",
    "seg_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden, 2)\n",
    "in_features = seg_model.roi_heads.box_predictor.cls_score.in_features\n",
    "seg_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "seg_model = seg_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in seg_model.parameters() if p.requires_grad], \n",
    "                      lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(6):\n",
    "    seg_model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(train_dl_seg, desc=f\"Epoch {epoch+1}/6\", leave=True)\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = seg_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/6 completed in {elapsed:.1f}s | Avg Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Mask R-CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ AUTOENCODER ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ AUTOENCODER - UNSUPERVISED LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CropOnly(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "\n",
    "class SmallAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1), nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.dec(self.enc(x))\n",
    "\n",
    "ae_ds = CropOnly(crop_dir)\n",
    "ae_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"üìä AutoEncoder dataset: {len(ae_ds)} crops\")\n",
    "\n",
    "ae = SmallAE().to(device)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(10):\n",
    "    ae.train()\n",
    "    tot = 0\n",
    "    for xb in ae_dl:\n",
    "        xb = xb.to(device)\n",
    "        recon = ae(xb)\n",
    "        loss = ((recon - xb)**2).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tot += loss.item() * xb.size(0)\n",
    "    print(f\"Epoch {epoch+1:2d}/10: MSE={tot/len(ae_ds):.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ AutoEncoder training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58963e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ GAN (DCGAN) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üëª GAN (DCGAN) - GENERATIVE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "nz, ngf, ndf = 64, 64, 64\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, 3, 4, 2, 1), nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z): \n",
    "        return self.net(z)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf*2, 4, 2, 1), nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1), nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf*4, 1, 4, 1, 0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "gen, disc = G().to(device), D().to(device)\n",
    "optG = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optD = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "gan_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {device}\\n\")\n",
    "for epoch in range(10):\n",
    "    for real in gan_dl:\n",
    "        real = real.to(device)\n",
    "        # Train D\n",
    "        z = torch.randn(real.size(0), nz, 1, 1, device=device)\n",
    "        fake = gen(z).detach()\n",
    "        d_real = disc(real)\n",
    "        d_fake = disc(fake)\n",
    "        lossD = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
    "        optD.zero_grad()\n",
    "        lossD.backward()\n",
    "        optD.step()\n",
    "        # Train G\n",
    "        z = torch.randn(real.size(0), nz, 1, 1, device=device)\n",
    "        fake = gen(z)\n",
    "        g = disc(fake)\n",
    "        lossG = bce(g, torch.ones_like(g))\n",
    "        optG.zero_grad()\n",
    "        lossG.backward()\n",
    "        optG.step()\n",
    "    print(f\"Epoch {epoch+1:2d}/10: D Loss={lossD.item():.4f} | G Loss={lossG.item():.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ GAN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: CNN RESULTS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé® DEMO: CNN Classification Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch, sample_labels = next(iter(val_dl_cnn))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    predictions = model(sample_batch)\n",
    "    predicted_classes = predictions.argmax(1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('CNN Classification Results (ResNet18)', fontsize=14, fontweight='bold')\n",
    "for idx in range(8):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    img = sample_batch[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    pred = predicted_classes[idx].item()\n",
    "    label = sample_labels[idx].item()\n",
    "    color = 'green' if pred == label else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {label}', color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'CNN_Results.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e375a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: FASTER R-CNN DETECTION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ DEMO: Faster R-CNN Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "det_model.eval()\n",
    "sample_imgs, sample_targets = next(iter(val_dl_det))\n",
    "sample_imgs_device = [im.to(device) for im in sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = det_model(sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Faster R-CNN Detection Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    ax = axes[idx]\n",
    "    img = sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Ground truth\n",
    "    for box in sample_targets[idx]['boxes'].cpu().numpy():\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Predictions\n",
    "    pred = predictions[idx]\n",
    "    for score, box in zip(pred['scores'].cpu().numpy(), pred['boxes'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_title(f'Image {idx+1}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'RCNN_Detection.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: MASK R-CNN SEGMENTATION ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≠ DEMO: Mask R-CNN Segmentation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "seg_model.eval()\n",
    "seg_sample_imgs, seg_sample_targets = next(iter(val_dl_seg))\n",
    "seg_sample_imgs_device = [im.to(device) for im in seg_sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_predictions = seg_model(seg_sample_imgs_device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Mask R-CNN Segmentation Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    # Ground truth\n",
    "    ax = axes[0, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Ground Truth - Image {idx+1}', fontweight='bold')\n",
    "    gt_masks = seg_sample_targets[idx]['masks'].cpu().numpy()\n",
    "    for mask in gt_masks:\n",
    "        ax.contour(mask, colors='green', linewidths=2)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    ax = axes[1, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Predictions - Image {idx+1}', fontweight='bold')\n",
    "    pred = seg_predictions[idx]\n",
    "    for mask, score in zip(pred['masks'].cpu().numpy(), pred['scores'].cpu().numpy()):\n",
    "        if score > 0.5:\n",
    "            ax.contour(mask.squeeze(), colors='red', linewidths=2, linestyles='--')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'MaskRCNN_Segmentation.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: AUTOENCODER ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ DEMO: AutoEncoder Reconstruction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ae.eval()\n",
    "sample_imgs_ae = next(iter(ae_dl))[:8].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = ae(sample_imgs_ae)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('AutoEncoder: Original vs Reconstructed', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    ax = axes[0, i]\n",
    "    img_orig = sample_imgs_ae[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_orig = np.clip(img_orig, 0, 1)\n",
    "    ax.imshow(img_orig)\n",
    "    ax.set_title('Original', fontsize=9)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    ax = axes[1, i]\n",
    "    img_recon = reconstructed[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_recon = np.clip(img_recon, 0, 1)\n",
    "    ax.imshow(img_recon)\n",
    "    ax.set_title('Reconstructed', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'AE_Reconstruction.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mse_errors = ((reconstructed - sample_imgs_ae)**2).mean(dim=[1,2,3]).cpu().numpy()\n",
    "    avg_mse = mse_errors.mean()\n",
    "print(f\"   Average MSE: {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ VISUALIZATION: GAN ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üëª DEMO: GAN Generated Images\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gen.eval()\n",
    "z_samples = torch.randn(16, nz, 1, 1, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = gen(z_samples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('DCGAN: Generated Synthetic Pedestrian Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(16):\n",
    "    ax = axes[idx // 8, idx % 8]\n",
    "    img = generated_images[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Generated {idx+1}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'GAN_Generated.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963935ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ SAVE MODELS ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models to output directory\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'model_cnn.pth'))\n",
    "torch.save(det_model.state_dict(), os.path.join(output_dir, 'model_faster_rcnn.pth'))\n",
    "torch.save(seg_model.state_dict(), os.path.join(output_dir, 'model_mask_rcnn.pth'))\n",
    "torch.save(ae.state_dict(), os.path.join(output_dir, 'model_autoencoder.pth'))\n",
    "torch.save(gen.state_dict(), os.path.join(output_dir, 'model_generator.pth'))\n",
    "torch.save(disc.state_dict(), os.path.join(output_dir, 'model_discriminator.pth'))\n",
    "\n",
    "print(f\"‚úÖ Models saved to: {output_dir}\")\n",
    "print(f\"   - model_cnn.pth\")\n",
    "print(f\"   - model_faster_rcnn.pth\")\n",
    "print(f\"   - model_mask_rcnn.pth\")\n",
    "print(f\"   - model_autoencoder.pth\")\n",
    "print(f\"   - model_generator.pth\")\n",
    "print(f\"   - model_discriminator.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll outputs saved to: {output_dir}\")\n",
    "print(\"\\nüìä Generated files:\")\n",
    "for f in glob.glob(os.path.join(output_dir, '*.png')):\n",
    "    print(f\"   - {os.path.basename(f)}\")\n",
    "for f in glob.glob(os.path.join(output_dir, '*.pth')):\n",
    "    print(f\"   - {os.path.basename(f)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
