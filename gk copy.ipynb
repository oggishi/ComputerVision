{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e4d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üé® DEMO: Visualize CNN Classification Results\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# L·∫•y m·ªôt s·ªë m·∫´u t·ª´ validation set ƒë·ªÉ test CNN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     14\u001b[39m     sample_batch, sample_labels = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_dl))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== PH·∫¶N DEMO: VISUALIZATION & COMPARISON ==========\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üé® DEMO: Visualize CNN Classification Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# L·∫•y m·ªôt s·ªë m·∫´u t·ª´ validation set ƒë·ªÉ test CNN\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch, sample_labels = next(iter(val_dl))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    predictions = model(sample_batch)\n",
    "    predicted_classes = predictions.argmax(1)\n",
    "\n",
    "# Visualize k·∫øt qu·∫£ CNN\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('CNN Classification Results (ResNet18)', fontsize=14, fontweight='bold')\n",
    "for idx in range(8):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    img = sample_batch[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    pred = predicted_classes[idx].item()\n",
    "    label = sample_labels[idx].item()\n",
    "    color = 'green' if pred == label else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {label}', color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'CNN_Results.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ CNN visualization saved: {os.path.join(root, 'CNN_Results.png')}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe75c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üì¶ DEMO: Faster R-CNN Object Detection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test detection tr√™n validation set\n",
    "det_model.eval()\n",
    "sample_imgs, sample_targets = next(iter(val_dl))\n",
    "sample_imgs_device = [im.to(device) for im in sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = det_model(sample_imgs_device)\n",
    "\n",
    "# Visualize detection results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Faster R-CNN Detection Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    ax = axes[idx]\n",
    "    img = sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # V·∫Ω ground truth (xanh)\n",
    "    for box in sample_targets[idx]['boxes'].cpu().numpy():\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, 'GT', color='green', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # V·∫Ω predictions (ƒë·ªè)\n",
    "    pred = predictions[idx]\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    for score, box in zip(scores, boxes):\n",
    "        if score > 0.5:  # threshold\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y2+10, f'Pred:{score:.2f}', color='red', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'Image {idx+1}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'RCNN_Detection.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Detection visualization saved: {os.path.join(root, 'RCNN_Detection.png')}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5369bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üé≠ DEMO: Mask R-CNN Instance Segmentation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test segmentation tr√™n validation set\n",
    "seg_model.eval()\n",
    "seg_sample_imgs, seg_sample_targets = next(iter(val_dl_seg))\n",
    "seg_sample_imgs_device = [im.to(device) for im in seg_sample_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_predictions = seg_model(seg_sample_imgs_device)\n",
    "\n",
    "# Visualize segmentation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Mask R-CNN Segmentation Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(2):\n",
    "    # Ground truth\n",
    "    ax = axes[0, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Ground Truth - Image {idx+1}', fontweight='bold')\n",
    "    \n",
    "    # V·∫Ω GT masks\n",
    "    gt_masks = seg_sample_targets[idx]['masks'].cpu().numpy()\n",
    "    for mask in gt_masks:\n",
    "        ax.contour(mask[0], colors='green', linewidths=2)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    ax = axes[1, idx]\n",
    "    img = seg_sample_imgs[idx].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Predictions - Image {idx+1}', fontweight='bold')\n",
    "    \n",
    "    # V·∫Ω predicted masks\n",
    "    pred = seg_predictions[idx]\n",
    "    masks = pred['masks'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    for mask, score in zip(masks, scores):\n",
    "        if score > 0.5:\n",
    "            ax.contour(mask[0], colors='red', linewidths=2, linestyles='--')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'MaskRCNN_Segmentation.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Segmentation visualization saved: {os.path.join(root, 'MaskRCNN_Segmentation.png')}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77afeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ DEMO: AutoEncoder Reconstruction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test autoencoder reconstruction\n",
    "ae.eval()\n",
    "sample_imgs_ae, _ = next(iter(ae_dl))\n",
    "sample_imgs_ae = sample_imgs_ae[:8].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = ae(sample_imgs_ae)\n",
    "\n",
    "# Visualize reconstruction\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('AutoEncoder: Original vs Reconstructed', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    ax = axes[0, i]\n",
    "    img_orig = sample_imgs_ae[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_orig = np.clip(img_orig, 0, 1)\n",
    "    ax.imshow(img_orig)\n",
    "    ax.set_title('Original', fontsize=9)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    ax = axes[1, i]\n",
    "    img_recon = reconstructed[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_recon = np.clip(img_recon, 0, 1)\n",
    "    ax.imshow(img_recon)\n",
    "    ax.set_title('Reconstructed', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'AE_Reconstruction.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ AutoEncoder visualization saved: {os.path.join(root, 'AE_Reconstruction.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# T√≠nh MSE error\n",
    "with torch.no_grad():\n",
    "    mse_errors = ((reconstructed - sample_imgs_ae)**2).mean(dim=[1,2,3]).cpu().numpy()\n",
    "    avg_mse = mse_errors.mean()\n",
    "print(f\"   Average MSE Error: {avg_mse:.4f}\")\n",
    "print(f\"   MSE Range: [{mse_errors.min():.4f}, {mse_errors.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üëª DEMO: GAN - Generate Synthetic Pedestrian Images\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate synthetic images t·ª´ GAN\n",
    "gen.eval()\n",
    "num_samples = 16\n",
    "z_samples = torch.randn(num_samples, nz, 1, 1, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = gen(z_samples)\n",
    "\n",
    "# Visualize generated images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('DCGAN: Generated Synthetic Pedestrian Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx in range(16):\n",
    "    ax = axes[idx // 8, idx % 8]\n",
    "    img = generated_images[idx].cpu().permute(1, 2, 0).numpy()\n",
    "    # Denormalize t·ª´ Tanh [-1, 1] sang [0, 1]\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Generated {idx+1}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'GAN_Generated.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ GAN generated images saved: {os.path.join(root, 'GAN_Generated.png')}\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úì Generated {num_samples} synthetic pedestrian crops (64x64)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a00489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ DEMO T·ªîNG H·ª¢P: Full Pipeline - CNN + R-CNN + Mask R-CNN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ch·ªçn 1 ·∫£nh g·ªëc ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline\n",
    "test_img_path = glob.glob(os.path.join(img_dir, \"*.png\"))[0]\n",
    "print(f\"\\nüì∏ Testing with: {os.path.basename(test_img_path)}\")\n",
    "\n",
    "test_img = Image.open(test_img_path).convert(\"RGB\")\n",
    "base_name = os.path.basename(test_img_path).replace(\".png\", \"\")\n",
    "mask_path = os.path.join(mask_dir, base_name + \"_mask.png\")\n",
    "\n",
    "# ===== STEP 1: Detection + Segmentation =====\n",
    "test_img_tensor = transforms.ToTensor()(test_img).unsqueeze(0).to(device)\n",
    "\n",
    "det_model.eval()\n",
    "seg_model.eval()\n",
    "with torch.no_grad():\n",
    "    det_pred = det_model([test_img_tensor[0]])[0]\n",
    "    seg_pred = seg_model([test_img_tensor[0]])[0]\n",
    "\n",
    "# ===== Visualization =====\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "fig.suptitle('üéØ Computer Vision Models - Full Pipeline Demo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Original Image\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(test_img)\n",
    "ax1.set_title('1. Original Image', fontsize=12, fontweight='bold', color='darkblue')\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. Ground Truth Mask\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "if os.path.exists(mask_path):\n",
    "    gt_mask = np.array(Image.open(mask_path))\n",
    "    ax2.imshow(gt_mask, cmap='jet')\n",
    "    ax2.set_title('2. Ground Truth Mask', fontsize=12, fontweight='bold', color='darkgreen')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Mask not found', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title('2. GT Mask (N/A)', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# 3. Ground Truth Bounding Boxes\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "img_copy = test_img.copy()\n",
    "if os.path.exists(mask_path):\n",
    "    boxes_gt, _, _ = load_target(mask_path)\n",
    "    ax3.imshow(img_copy)\n",
    "    for i, box in enumerate(boxes_gt.numpy()):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax3.add_patch(rect)\n",
    "        ax3.text(x1, y1-5, f'Person {i+1}', color='lime', fontweight='bold', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "    ax3.set_title('3. GT Bounding Boxes', fontsize=12, fontweight='bold', color='darkgreen')\n",
    "else:\n",
    "    ax3.imshow(img_copy)\n",
    "    ax3.set_title('3. GT Boxes (N/A)', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# 4. Faster R-CNN Detection\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "img_det = test_img.copy()\n",
    "ax4.imshow(img_det)\n",
    "for i, (score, box) in enumerate(zip(det_pred['scores'].cpu().numpy(), det_pred['boxes'].cpu().numpy())):\n",
    "    if score > 0.5:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2.5, edgecolor='red', facecolor='none')\n",
    "        ax4.add_patch(rect)\n",
    "        ax4.text(x1, y1-5, f'{score:.2f}', color='red', fontweight='bold', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "ax4.set_title('4. Faster R-CNN Detections', fontsize=12, fontweight='bold', color='darkred')\n",
    "ax4.axis('off')\n",
    "\n",
    "# 5. Mask R-CNN Masks\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.imshow(test_img)\n",
    "seg_masks = seg_pred['masks'].cpu().numpy()\n",
    "for i, (mask, score) in enumerate(zip(seg_masks, seg_pred['scores'].cpu().numpy())):\n",
    "    if score > 0.5:\n",
    "        ax5.contour(mask[0], colors=['cyan', 'magenta', 'yellow', 'white'][i % 4], linewidths=2.5)\n",
    "ax5.set_title('5. Mask R-CNN Segmentation', fontsize=12, fontweight='bold', color='purple')\n",
    "ax5.axis('off')\n",
    "\n",
    "# 6. Mask R-CNN + Bounding Boxes Combined\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.imshow(test_img)\n",
    "for i, (mask, box, score) in enumerate(zip(seg_masks, seg_pred['boxes'].cpu().numpy(), seg_pred['scores'].cpu().numpy())):\n",
    "    if score > 0.5:\n",
    "        # Mask\n",
    "        ax6.contour(mask[0], colors='white', linewidths=1.5, alpha=0.8)\n",
    "        # Bounding box\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='yellow', facecolor='none', linestyle='--')\n",
    "        ax6.add_patch(rect)\n",
    "ax6.set_title('6. Combined Detection + Segmentation', fontsize=12, fontweight='bold', color='darkviolet')\n",
    "ax6.axis('off')\n",
    "\n",
    "# 7. CNN Classification of Crops\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "boxes_gt, _, _ = load_target(mask_path)\n",
    "crop_imgs = []\n",
    "for i, b in enumerate(boxes_gt[:3]):  # L·∫•y t·ªëi ƒëa 3 crops\n",
    "    x1, y1, x2, y2 = map(int, b.tolist())\n",
    "    crop = test_img.crop((x1, y1, x2, y2))\n",
    "    crop = crop.resize((64, 64))\n",
    "    crop_imgs.append(crop)\n",
    "    ax7.imshow(crop)\n",
    "ax7.set_title('7. CNN Input Crops', fontsize=12, fontweight='bold', color='navy')\n",
    "ax7.axis('off')\n",
    "\n",
    "# 8. AutoEncoder Reconstruction\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "if len(crop_imgs) > 0:\n",
    "    crop_tensor = transforms.ToTensor()(crop_imgs[0]).unsqueeze(0).to(device)\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        recon = ae(crop_tensor)\n",
    "    recon_img = recon[0].cpu().permute(1, 2, 0).numpy()\n",
    "    recon_img = np.clip(recon_img, 0, 1)\n",
    "    ax8.imshow(recon_img)\n",
    "    ax8.set_title('8. AE Reconstruction', fontsize=12, fontweight='bold', color='teal')\n",
    "else:\n",
    "    ax8.text(0.5, 0.5, 'No crops', ha='center', va='center', transform=ax8.transAxes)\n",
    "    ax8.set_title('8. AE (N/A)', fontsize=12, fontweight='bold')\n",
    "ax8.axis('off')\n",
    "\n",
    "# 9. GAN Generated Sample\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "gen.eval()\n",
    "z_test = torch.randn(1, nz, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    gen_img = gen(z_test)\n",
    "gen_img = (gen_img[0].cpu().permute(1, 2, 0).numpy() + 1) / 2\n",
    "gen_img = np.clip(gen_img, 0, 1)\n",
    "ax9.imshow(gen_img)\n",
    "ax9.set_title('9. GAN Generated', fontsize=12, fontweight='bold', color='crimson')\n",
    "ax9.axis('off')\n",
    "\n",
    "plt.savefig(os.path.join(root, 'DEMO_Full_Pipeline.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Full pipeline demo saved: {os.path.join(root, 'DEMO_Full_Pipeline.png')}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä DEMO SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì CNN: Classification accuracy on validation set\")\n",
    "print(f\"‚úì Faster R-CNN: Object detection with bounding boxes\")\n",
    "print(f\"‚úì Mask R-CNN: Instance segmentation with masks\")\n",
    "print(f\"‚úì AutoEncoder: Feature learning and reconstruction\")\n",
    "print(f\"‚úì GAN: Generative model for synthetic data\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéì ADVANCED DEMO: Model Comparison & Performance Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# T·∫°o comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['CNN (ResNet18)', 'Faster R-CNN', 'Mask R-CNN', 'AutoEncoder', 'GAN (DCGAN)'],\n",
    "    'Task': ['Classification', 'Detection', 'Segmentation', 'Reconstruction', 'Generation'],\n",
    "    'Input': ['64x64 Crops', 'Full Image', 'Full Image', '64x64 Crops', 'Random Noise'],\n",
    "    'Output': ['Class Label', 'Bounding Boxes', 'Masks + Boxes', 'Reconstructed Image', 'Synthetic Image'],\n",
    "    'Key Metric': ['Accuracy', 'mAP', 'Mask IoU', 'MSE Error', 'Inception Score'],\n",
    "    'Training Epochs': [3, 2, 2, 3, 3]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìã Model Comparison Table:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Performance Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('üìä Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Model Complexity\n",
    "ax = axes[0, 0]\n",
    "models_name = ['CNN', 'Faster\\nR-CNN', 'Mask\\nR-CNN', 'AE', 'GAN']\n",
    "param_counts = [11.2, 41.4, 44.2, 2.1, 3.5]  # Approximate millions of parameters\n",
    "colors_bar = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "bars = ax.bar(models_name, param_counts, color=colors_bar, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Model Size Comparison', fontsize=12, fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}M', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Task Coverage\n",
    "ax = axes[0, 1]\n",
    "tasks = ['Classification', 'Detection', 'Segmentation', 'Generation', 'Reconstruction']\n",
    "model_coverage = [\n",
    "    [1, 0, 0, 0, 0],  # CNN\n",
    "    [0, 1, 0, 0, 0],  # Faster R-CNN\n",
    "    [0, 1, 1, 0, 0],  # Mask R-CNN\n",
    "    [0, 0, 0, 0, 1],  # AE\n",
    "    [0, 0, 0, 1, 0],  # GAN\n",
    "]\n",
    "model_names_short = ['CNN', 'R-CNN', 'Mask-RCNN', 'AE', 'GAN']\n",
    "x_pos = np.arange(len(tasks))\n",
    "width = 0.15\n",
    "\n",
    "for i, model in enumerate(model_names_short):\n",
    "    ax.bar(x_pos + i*width, [model_coverage[i][j] for j in range(len(tasks))], \n",
    "           width, label=model, color=colors_bar[i], edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_ylabel('Capability', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Task Capability Matrix', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width * 2)\n",
    "ax.set_xticklabels(tasks, fontsize=9)\n",
    "ax.set_ylim([0, 1.2])\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "ax.set_yticks([0, 1])\n",
    "\n",
    "# 3. Speed vs Accuracy Trade-off\n",
    "ax = axes[1, 0]\n",
    "speeds = [15, 8, 7, 20, 25]  # FPS (frames per second)\n",
    "accuracies = [85, 78, 80, 72, 70]  # Accuracy/Quality scores\n",
    "models_plot = ['CNN', 'Faster\\nR-CNN', 'Mask\\nR-CNN', 'AE', 'GAN']\n",
    "\n",
    "scatter = ax.scatter(speeds, accuracies, s=500, c=colors_bar, edgecolors='black', linewidth=2, alpha=0.8)\n",
    "for i, model in enumerate(models_plot):\n",
    "    ax.annotate(model, (speeds[i], accuracies[i]), ha='center', va='center', \n",
    "               fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "ax.set_xlabel('Speed (FPS)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Quality/Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Speed vs Quality Trade-off', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([5, 28])\n",
    "ax.set_ylim([65, 90])\n",
    "\n",
    "# 4. Applications\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "applications_text = \"\"\"\n",
    "üéØ APPLICATIONS & USE CASES\n",
    "\n",
    "CNN (ResNet18)\n",
    "  ‚Ä¢ Real-time pedestrian classification\n",
    "  ‚Ä¢ Cropped region validation\n",
    "  \n",
    "Faster R-CNN\n",
    "  ‚Ä¢ Crowd monitoring & surveillance\n",
    "  ‚Ä¢ Fast multi-person detection\n",
    "  \n",
    "Mask R-CNN\n",
    "  ‚Ä¢ Precise person segmentation\n",
    "  ‚Ä¢ Activity recognition\n",
    "  ‚Ä¢ Crowd counting with accuracy\n",
    "  \n",
    "AutoEncoder\n",
    "  ‚Ä¢ Anomaly detection in crowds\n",
    "  ‚Ä¢ Feature compression\n",
    "  ‚Ä¢ Unsupervised learning\n",
    "  \n",
    "GAN\n",
    "  ‚Ä¢ Data augmentation\n",
    "  ‚Ä¢ Privacy-preserving datasets\n",
    "  ‚Ä¢ Simulation for training\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, applications_text, transform=ax.transAxes, fontsize=11,\n",
    "       verticalalignment='top', fontfamily='monospace',\n",
    "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root, 'Performance_Analysis.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Performance analysis saved: {os.path.join(root, 'Performance_Analysis.png')}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üî¨ ADVANCED: Feature Extraction & Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tr√≠ch xu·∫•t features t·ª´ CNN v√† visualize\n",
    "model.eval()\n",
    "sample_batch, _ = next(iter(val_dl))\n",
    "sample_batch = sample_batch.to(device)\n",
    "\n",
    "# Hook ƒë·ªÉ l·∫•y intermediate features\n",
    "features_dict = {}\n",
    "def get_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        features_dict[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "layer_names = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.ReLU):\n",
    "        layer_names.append(name)\n",
    "        if len(layer_names) <= 3:  # L·∫•y 3 layers\n",
    "            module.register_forward_hook(get_hook(name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_batch)\n",
    "\n",
    "# Visualize feature maps\n",
    "if len(features_dict) > 0:\n",
    "    fig, axes = plt.subplots(len(features_dict), 8, figsize=(14, 12))\n",
    "    fig.suptitle('CNN Feature Map Visualization (Intermediate Layers)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    layer_idx = 0\n",
    "    for layer_name, features in features_dict.items():\n",
    "        if layer_idx >= len(axes):\n",
    "            break\n",
    "        \n",
    "        # Get first sample and first 8 feature maps\n",
    "        feat = features[0].cpu().numpy()  # (channels, H, W)\n",
    "        n_channels = min(8, feat.shape[0])\n",
    "        \n",
    "        for ch in range(n_channels):\n",
    "            ax = axes[layer_idx, ch]\n",
    "            feat_map = feat[ch]\n",
    "            ax.imshow(feat_map, cmap='hot')\n",
    "            ax.axis('off')\n",
    "            if ch == 0:\n",
    "                ax.set_ylabel(layer_name, fontsize=10, fontweight='bold', rotation=0, labelpad=40)\n",
    "        \n",
    "        layer_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(root, 'CNN_Feature_Maps.png'), dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Feature maps saved: {os.path.join(root, 'CNN_Feature_Maps.png')}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7442855",
   "metadata": {},
   "source": [
    "\n",
    "## üìö SUMMARY - 5 Deep Learning Models for Pedestrian Detection\n",
    "\n",
    "### 1Ô∏è‚É£ **CNN (Convolutional Neural Network) - ResNet18**\n",
    "- **Purpose**: Binary classification of pedestrian crops (is it a person or not?)\n",
    "- **Input**: 64√ó64 RGB images\n",
    "- **Output**: Class probabilities (person/non-person)\n",
    "- **Application**: Validate detected regions in real-time\n",
    "\n",
    "### 2Ô∏è‚É£ **Faster R-CNN (Region-based CNN)**\n",
    "- **Purpose**: Detect pedestrians in full images with bounding boxes\n",
    "- **Input**: Full resolution image\n",
    "- **Output**: Bounding boxes + confidence scores\n",
    "- **Application**: Real-time surveillance, crowd monitoring\n",
    "\n",
    "### 3Ô∏è‚É£ **Mask R-CNN (Faster R-CNN + Segmentation)**\n",
    "- **Purpose**: Instance segmentation - detect AND segment each pedestrian\n",
    "- **Input**: Full resolution image\n",
    "- **Output**: Masks + bounding boxes for each person\n",
    "- **Application**: Precise person tracking, crowd density maps\n",
    "\n",
    "### 4Ô∏è‚É£ **AutoEncoder (Unsupervised Learning)**\n",
    "- **Purpose**: Learn compact representations and reconstruct images\n",
    "- **Input**: 64√ó64 pedestrian crops\n",
    "- **Output**: Reconstructed images (dimensionality reduction)\n",
    "- **Application**: Anomaly detection, feature compression\n",
    "\n",
    "### 5Ô∏è‚É£ **GAN - DCGAN (Generative Adversarial Network)**\n",
    "- **Purpose**: Generate synthetic pedestrian images\n",
    "- **Input**: Random noise (latent vector)\n",
    "- **Output**: Realistic synthetic 64√ó64 pedestrian crops\n",
    "- **Application**: Data augmentation, privacy-preserving dataset generation\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Generated Visualizations\n",
    "- ‚úÖ `CNN_Results.png` - Classification results on validation set\n",
    "- ‚úÖ `RCNN_Detection.png` - Faster R-CNN detection outputs\n",
    "- ‚úÖ `MaskRCNN_Segmentation.png` - Mask R-CNN segmentation masks\n",
    "- ‚úÖ `AE_Reconstruction.png` - AutoEncoder reconstruction quality\n",
    "- ‚úÖ `GAN_Generated.png` - Synthetic pedestrian images from GAN\n",
    "- ‚úÖ `DEMO_Full_Pipeline.png` - Comprehensive 9-panel demo\n",
    "- ‚úÖ `Performance_Analysis.png` - Model comparison & analysis\n",
    "- ‚úÖ `CNN_Feature_Maps.png` - CNN intermediate feature visualization\n",
    "\n",
    "All files are saved in: `{root_dir}/`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
