{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# ğŸ”¹ CHá»ˆ Cáº¦N Äá»”I Ä‘Æ°á»ng dáº«n táº¡i Ä‘Ã¢y theo nÆ¡i báº¡n lÆ°u dataset\n",
    "root = r\"D:/datasets/PennFudanPed\"   # <--- Ä‘á»•i thÃ nh Ä‘Æ°á»ng dáº«n thá»±c táº¿ trÃªn mÃ¡y báº¡n\n",
    "img_dir = os.path.join(root, \"PNGImages\")\n",
    "mask_dir = os.path.join(root, \"PedMasks\")\n",
    "\n",
    "# === HÃ m xá»­ lÃ½ máº·t náº¡ Ä‘á»ƒ láº¥y há»™p bao vÃ  mask ===\n",
    "def load_target(mask_p):\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    obj_ids = np.unique(mask)[1:]  # loáº¡i bá» background = 0\n",
    "    masks = (mask[..., None] == obj_ids).astype(np.uint8).transpose(2,0,1)\n",
    "    boxes = []\n",
    "    for m in masks:\n",
    "        pos = np.argwhere(m)\n",
    "        y1, x1 = pos.min(0)\n",
    "        y2, x2 = pos.max(0)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((len(boxes),), dtype=torch.int64)  # class=1 (person)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return boxes, labels, masks\n",
    "\n",
    "# === Táº¡o thÆ° má»¥c crops Ä‘á»ƒ lÆ°u áº£nh cáº¯t 64x64 dÃ¹ng cho CNN/AE/GAN ===\n",
    "crop_dir = os.path.join(root, \"crops64\")\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64,64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# === Duyá»‡t qua tá»«ng áº£nh vÃ  cáº¯t ngÆ°á»i theo mask ===\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(crop_dir, f\"{base}_{i}.png\"))\n",
    "\n",
    "print(f\"âœ… ÄÃ£ táº¡o áº£nh cáº¯t trong thÆ° má»¥c: {crop_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "class PedCropDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "        y = 1  # person\n",
    "        return x, y\n",
    "\n",
    "ds = PedCropDataset(crop_dir)\n",
    "n = len(ds); n_train = int(0.8*n)\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n-n_train])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(weights=None, num_classes=2).to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for xb,yb in train_dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot,correct = 0,0\n",
    "        for xb,yb in val_dl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            tot += yb.numel(); correct += (pred==yb).sum().item()\n",
    "    print(f\"Epoch {epoch+1}: val acc={correct/tot:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b013c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "class PennFudanDet(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, train=True):\n",
    "        self.imgs = sorted(glob.glob(os.path.join(img_dir,\"*.png\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.train = train\n",
    "        self.tf = transforms.ToTensor()\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\",\"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base+\"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "full = PennFudanDet(img_dir, mask_dir)\n",
    "n = len(full); n_train = int(0.8*n)\n",
    "train_ds, val_ds = torch.utils.data.random_split(full, [n_train, n-n_train])\n",
    "\n",
    "def collate(batch): \n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl   = DataLoader(val_ds, batch_size=2, collate_fn=collate)\n",
    "\n",
    "det_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = det_model.roi_heads.box_predictor.cls_score.in_features\n",
    "det_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "det_model = det_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in det_model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    det_model.train()\n",
    "    for imgs, targets in train_dl:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = det_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"Epoch {epoch+1}: train loss={loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d472e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "class PennFudanSeg(PennFudanDet):\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\",\"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base+\"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return transforms.ToTensor()(img), {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "train_ds_seg, val_ds_seg = torch.utils.data.random_split(PennFudanSeg(img_dir, mask_dir), [n_train, n-n_train])\n",
    "train_dl_seg = DataLoader(train_ds_seg, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "\n",
    "seg_model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "# replace the mask head for 2 classes (background + person)\n",
    "in_features_mask = seg_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden = 256\n",
    "seg_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden, 2)\n",
    "# replace box predictor too:\n",
    "in_features = seg_model.roi_heads.box_predictor.cls_score.in_features\n",
    "seg_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "seg_model = seg_model.to(device)\n",
    "\n",
    "opt = torch.optim.SGD([p for p in seg_model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    seg_model.train()\n",
    "    for imgs, targets in train_dl_seg:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = seg_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"[Mask R-CNN] Epoch {epoch+1}: train loss={loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9964d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CropOnly(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        return self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "\n",
    "ae_ds = CropOnly(crop_dir)\n",
    "ae_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "class SmallAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3,32,4,2,1), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4,2,1), nn.ReLU(),\n",
    "            nn.Conv2d(64,128,4,2,1), nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32,3,4,2,1), nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x): return self.dec(self.enc(x))\n",
    "\n",
    "ae = SmallAE().to(device)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "for epoch in range(3):\n",
    "    ae.train()\n",
    "    tot=0\n",
    "    for xb in ae_dl:\n",
    "        xb = xb.to(device)\n",
    "        recon = ae(xb)\n",
    "        loss = ((recon - xb)**2).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item()*xb.size(0)\n",
    "    print(f\"AE epoch {epoch+1}: MSE={tot/len(ae_ds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "nz, ngf, ndf = 64, 64, 64\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4,1,0), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4,2,1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4,2,1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, 3,      4,2,1), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self,z): return self.net(z)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf*2,4,2,1), nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf*2, ndf*4,4,2,1), nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf*4, 1, 4,1,0)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x).view(-1)\n",
    "\n",
    "gen, disc = G().to(device), D().to(device)\n",
    "optG = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "optD = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "gan_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "for epoch in range(3):\n",
    "    for real in gan_dl:\n",
    "        real = real.to(device)\n",
    "        # Train D\n",
    "        z = torch.randn(real.size(0), nz,1,1, device=device)\n",
    "        fake = gen(z).detach()\n",
    "        d_real = disc(real); d_fake = disc(fake)\n",
    "        lossD = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
    "        optD.zero_grad(); lossD.backward(); optD.step()\n",
    "        # Train G\n",
    "        z = torch.randn(real.size(0), nz,1,1, device=device)\n",
    "        fake = gen(z)\n",
    "        g = disc(fake)\n",
    "        lossG = bce(g, torch.ones_like(g))\n",
    "        optG.zero_grad(); lossG.backward(); optG.step()\n",
    "    print(f\"DCGAN epoch {epoch+1}: D={lossD.item():.3f}, G={lossG.item():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
