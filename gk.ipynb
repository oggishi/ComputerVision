{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b932ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã tạo ảnh cắt trong thư mục: D:/datasets/PennFudanPed\\crops64\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "root = r\"D:/datasets/PennFudanPed\"  \n",
    "img_dir = os.path.join(root, \"PNGImages\")\n",
    "mask_dir = os.path.join(root, \"PedMasks\")\n",
    "\n",
    "# === Hàm xử lý mặt nạ để lấy hộp bao và mask ===\n",
    "def load_target(mask_p):\n",
    "    mask = np.array(Image.open(mask_p))\n",
    "    obj_ids = np.unique(mask)[1:]  # loại bỏ background = 0\n",
    "    masks = (mask[..., None] == obj_ids).astype(np.uint8).transpose(2,0,1)\n",
    "    boxes = []\n",
    "    for m in masks:\n",
    "        pos = np.argwhere(m)\n",
    "        y1, x1 = pos.min(0)\n",
    "        y2, x2 = pos.max(0)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((len(boxes),), dtype=torch.int64)  # class=1 (person)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return boxes, labels, masks\n",
    "\n",
    "# === Tạo thư mục crops để lưu ảnh cắt 64x64 dùng cho CNN/AE/GAN ===\n",
    "crop_dir = os.path.join(root, \"crops64\")\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize64 = transforms.Resize((64,64), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# === Duyệt qua từng ảnh và cắt người theo mask ===\n",
    "for img_p in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "    base = os.path.basename(img_p).replace(\".png\", \"\")\n",
    "    mask_p = os.path.join(mask_dir, base + \"_mask.png\")\n",
    "    if not os.path.exists(mask_p):\n",
    "        continue\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    boxes, _, _ = load_target(mask_p)\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, b.tolist())\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crop = resize64(crop)\n",
    "        crop.save(os.path.join(crop_dir, f\"{base}_{i}.png\"))\n",
    "\n",
    "print(f\"✅ Đã tạo ảnh cắt trong thư mục: {crop_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d99356e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(ds); n_train = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m*n)\n\u001b[32m     16\u001b[39m train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n-n_train])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m train_dl = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m val_dl   = DataLoader(val_ds, batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m     20\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "class PedCropDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "        y = 1  # person\n",
    "        return x, y\n",
    "\n",
    "ds = PedCropDataset(crop_dir)\n",
    "n = len(ds); n_train = int(0.8*n)\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n-n_train])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(weights=None, num_classes=2).to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for xb,yb in train_dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot,correct = 0,0\n",
    "        for xb,yb in val_dl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            tot += yb.numel(); correct += (pred==yb).sum().item()\n",
    "    print(f\"Epoch {epoch+1}: val acc={correct/tot:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "class PennFudanDet(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, train=True):\n",
    "        self.imgs = sorted(glob.glob(os.path.join(img_dir,\"*.png\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.train = train\n",
    "        self.tf = transforms.ToTensor()\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\",\"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base+\"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return self.tf(img), {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "full = PennFudanDet(img_dir, mask_dir)\n",
    "n = len(full); n_train = int(0.8*n)\n",
    "train_ds, val_ds = torch.utils.data.random_split(full, [n_train, n-n_train])\n",
    "\n",
    "def collate(batch): \n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "val_dl   = DataLoader(val_ds, batch_size=2, collate_fn=collate)\n",
    "\n",
    "det_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = det_model.roi_heads.box_predictor.cls_score.in_features\n",
    "det_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "det_model = det_model.to(device)\n",
    "opt = torch.optim.SGD([p for p in det_model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    det_model.train()\n",
    "    for imgs, targets in train_dl:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = det_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"Epoch {epoch+1}: train loss={loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9810a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "class PennFudanSeg(PennFudanDet):\n",
    "    def __getitem__(self, i):\n",
    "        img_p = self.imgs[i]\n",
    "        base = os.path.basename(img_p).replace(\".png\",\"\")\n",
    "        mask_p = os.path.join(self.mask_dir, base+\"_mask.png\")\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        boxes, labels, masks = load_target(mask_p)\n",
    "        return transforms.ToTensor()(img), {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "train_ds_seg, val_ds_seg = torch.utils.data.random_split(PennFudanSeg(img_dir, mask_dir), [n_train, n-n_train])\n",
    "train_dl_seg = DataLoader(train_ds_seg, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "\n",
    "seg_model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "# replace the mask head for 2 classes (background + person)\n",
    "in_features_mask = seg_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden = 256\n",
    "seg_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden, 2)\n",
    "# replace box predictor too:\n",
    "in_features = seg_model.roi_heads.box_predictor.cls_score.in_features\n",
    "seg_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
    "seg_model = seg_model.to(device)\n",
    "\n",
    "opt = torch.optim.SGD([p for p in seg_model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    seg_model.train()\n",
    "    for imgs, targets in train_dl_seg:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = seg_model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"[Mask R-CNN] Epoch {epoch+1}: train loss={loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3721586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CropOnly(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        self.tf = transforms.Compose([transforms.ToTensor()])\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        return self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
    "\n",
    "ae_ds = CropOnly(crop_dir)\n",
    "ae_dl = DataLoader(ae_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "class SmallAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3,32,4,2,1), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4,2,1), nn.ReLU(),\n",
    "            nn.Conv2d(64,128,4,2,1), nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32,3,4,2,1), nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x): return self.dec(self.enc(x))\n",
    "\n",
    "ae = SmallAE().to(device)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "for epoch in range(3):\n",
    "    ae.train()\n",
    "    tot=0\n",
    "    for xb in ae_dl:\n",
    "        xb = xb.to(device)\n",
    "        recon = ae(xb)\n",
    "        loss = ((recon - xb)**2).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item()*xb.size(0)\n",
    "    print(f\"AE epoch {epoch+1}: MSE={tot/len(ae_ds):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
